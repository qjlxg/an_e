#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å›æµ‹æ¨¡å—ï¼šå¯¹ fund_data ç›®å½•ä¸‹æ‰€æœ‰åŸºé‡‘è¿›è¡Œæ»šåŠ¨ 30 å¤©çª—å£å›æµ‹
è¾“å‡ºï¼š
    1. backtest_results/YYYYMM/backtest_summary_*.csv   ï¼ˆæ¯åªåŸºé‡‘æ¯ä¸€å¤©çš„ä¿¡å· + åç»­æ”¶ç›Šï¼‰
    2. backtest_results/YYYYMM/backtest_report_*.md    ï¼ˆæ±‡æ€»ç»Ÿè®¡ï¼šèƒœç‡ã€å¹³å‡æ”¶ç›Šã€æœ€å¤§å›æ’¤ç­‰ï¼‰
"""

import os
import glob
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pytz
import yaml
import logging
from concurrent.futures import ProcessPoolExecutor
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings("ignore")

# ================================
# æ—¥å¿—
# ================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ================================
# åŠ è½½é…ç½®
# ================================
CONFIG_PATH = "config_backtest.yaml"
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    cfg = yaml.safe_load(f)

DATA_DIR = cfg["data_dir"]
OUTPUT_ROOT = cfg["output_dir"]
START_DATE = cfg.get("start_date")
END_DATE = cfg.get("end_date")
TH = cfg["thresholds"]
FORWARD_DAYS = cfg["forward_days"]
BENCH_DAYS = cfg["benchmark_hold_days"]
MIN_HISTORY = cfg["min_history_days"]
MAX_WORKERS = cfg.get("max_workers", 4)

# ================================
# æ ¸å¿ƒæŒ‡æ ‡å‡½æ•°ï¼ˆä¸ä¸»è„šæœ¬å®Œå…¨ä¸€è‡´ï¼‰
# ================================
def calculate_consecutive_drops(series: pd.Series) -> int:
    """ä»æœ€æ–°ä¸€å¤©å¼€å§‹è¿ç»­ä¸‹è·Œå¤©æ•°ï¼ˆåŒ…å«ä»Šå¤©ï¼‰"""
    if len(series) < 2:
        return 0
    values = series.values
    drops = values[:-1] > values[1:]
    count = 0
    for d in drops:
        if d:
            count += 1
        else:
            break
    return count


def calculate_max_drawdown(series: pd.Series) -> float:
    if series.empty:
        return 0.0
    peak = series.cummax()
    drawdown = (peak - series) / peak
    return drawdown.max()


def calculate_technical_indicators(df: pd.DataFrame) -> Dict[str, Any]:
    """è®¡ç®— RSIã€MACDã€MA50ã€å¸ƒæ—å¸¦ã€å½“æ—¥è·Œå¹…ï¼ˆä¸ä¸»è„šæœ¬ä¸€è‡´ï¼‰"""
    if 'value' not in df.columns or len(df) < 50:
        return {
            'RSI': np.nan, 'MACDä¿¡å·': 'æ•°æ®ä¸è¶³', 'å‡€å€¼/MA50': np.nan,
            'å¸ƒæ—å¸¦ä½ç½®': 'æ•°æ®ä¸è¶³', 'æœ€æ–°å‡€å€¼': np.nan, 'å½“æ—¥è·Œå¹…': np.nan
        }

    df_asc = df.iloc[::-1].copy()

    # RSI
    delta = df_asc['value'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = -delta.where(delta < 0, 0).rolling(14).mean()
    loss_safe = loss.replace(0, np.nan)
    rs = gain / loss_safe
    rsi_series = 100 - (100 / (1 + rs)).fillna(100)
    rsi_latest = rsi_series.iloc[-1]

    # MACD
    ema12 = df_asc['value'].ewm(span=12, adjust=False).mean()
    ema26 = df_asc['value'].ewm(span=26, adjust=False).mean()
    macd = ema12 - ema26
    signal = macd.ewm(span=9, adjust=False).mean()
    macd_latest = macd.iloc[-1]
    signal_latest = signal.iloc[-1]
    macd_prev = macd.iloc[-2] if len(macd) >= 2 else np.nan
    signal_prev = signal.iloc[-2] if len(signal) >= 2 else np.nan

    macd_signal = 'è§‚å¯Ÿ'
    if not np.isnan(macd_prev) and not np.isnan(signal_prev):
        if macd_latest > signal_latest and macd_prev <= signal_prev:
            macd_signal = 'é‡‘å‰'
        elif macd_latest < signal_latest and  and macd_prev >= signal_prev:
            macd_signal = 'æ­»å‰'

    # MA50
    ma50 = df_asc['value'].rolling(50).mean()
    ma50_latest = ma50.iloc[-1]
    value_latest = df_asc['value'].iloc[-1]
    net_to_ma50 = value_latest / ma50_latest if ma50_latest != 0 else np.nan

    # å¸ƒæ—å¸¦
    ma20 = df_asc['value'].rolling(20).mean()
    std20 = df_asc['value'].rolling(20).std()
    ma20_latest = ma20.iloc[-1]
    std_latest = std20.iloc[-1]
    bollinger_pos = 'æ•°æ®ä¸è¶³'
    if not np.isnan(ma20_latest) and not np.isnan(std_latest) and std_latest > 0:
        upper = ma20_latest + 2 * std_latest
        lower = ma20_latest - 2 * std_latest
        if value_latest > upper:
            bollinger_pos = 'ä¸Šè½¨ä¸Šæ–¹'
        elif value_latest < lower:
            bollinger_pos = 'ä¸‹è½¨ä¸‹æ–¹'
        elif value_latest > ma20_latest:
            bollinger_pos = 'ä¸­è½¨ä¸Šæ–¹'
        else:
            bollinger_pos = 'ä¸­è½¨ä¸‹æ–¹/ä¸­è½¨'

    # å½“æ—¥è·Œå¹…
    daily_drop = 0.0
    if len(df_asc) >= 2:
        prev = df_asc['value'].iloc[-2]
        if prev > 0:
            daily_drop = (prev - value_latest) / prev

    return {
        'RSI': round(rsi_latest, 2) if not np.isnan(rsi_latest) else np.nan,
        'MACDä¿¡å·': macd_signal,
        'å‡€å€¼/MA50': round(net_to_ma50, 2) if not np.isnan(net_to_ma50) else np.nan,
        'å¸ƒæ—å¸¦ä½ç½®': bollinger_pos,
        'æœ€æ–°å‡€å€¼': round(value_latest, 4),
        'å½“æ—¥è·Œå¹…': round(daily_drop, 4)
    }


def generate_signal(row: pd.Series) -> str:
    """ç»Ÿä¸€ç”Ÿæˆä¿¡å·ç­‰çº§ï¼ˆğŸ¥‡ğŸ¥ˆğŸ¥‰ï¼‰"""
    if (row['æœ€å¤§å›æ’¤'] >= TH["high_elasticity_min_drawdown"] and
        row['è¿‘ä¸€å‘¨è¿è·Œ'] == 1 and
        not pd.isna(row['RSI'])):

        if row['RSI'] < TH["rsi_extreme_oversold"] and row['å½“æ—¥è·Œå¹…'] >= TH["min_daily_drop_percent"]:
            return "ğŸ¥‡ å³æ—¶ä¹°å…¥"
        if row['RSI'] < TH["rsi_oversold"] and row['å½“æ—¥è·Œå¹…'] >= TH["min_daily_drop_percent"]:
            return "ğŸ¥‡ å³æ—¶ä¹°å…¥"
        if row['RSI'] < TH["rsi_oversold"]:
            return "ğŸ¥ˆ æŠ€æœ¯å»ºä»“"
        return "ğŸ¥‰ è§‚å¯Ÿæ± "
    return "æ— ä¿¡å·"


# ================================
# å•åŸºé‡‘å›æµ‹å‡½æ•°ï¼ˆä¾›è¿›ç¨‹æ± è°ƒç”¨ï¼‰
# ================================
def backtest_single_fund(filepath: str) -> List[Dict]:
    fund_code = os.path.splitext(os.path.basename(filepath))[0]
    try:
        df = pd.read_csv(filepath, parse_dates=['date'])
        if 'net_value' in df.columns:
            df = df.rename(columns={'net_value': 'value'})
        elif 'value' not in df.columns:
            logger.warning(f"{fund_code} æ— å‡€å€¼åˆ—")
            return []

        df = df[['date', 'value']].dropna().sort_values('date').reset_index(drop=True)
        if len(df) < MIN_HISTORY:
            return []

        # æ—¶é—´è¿‡æ»¤
        if START_DATE:
            df = df[df['date'] >= pd.to_datetime(START_DATE)]
        if END_DATE:
            df = df[df['date'] <= pd.to_datetime(END_DATE)]
        if len(df) < 60:
            return []

        records = []
        # æ»šåŠ¨çª—å£ï¼šæ¯ä¸€å¤©ä½œä¸º T æ—¥
        for i in range(30, len(df)):
            window = df.iloc[i-30:i].copy()                # æœ€è¿‘30å¤©
            week = df.iloc[i-5:i].copy() if i >= 5 else window.iloc[-5:]

            # æ ¸å¿ƒæŒ‡æ ‡
            max_drop_month = calculate_consecutive_drops(window['value'])
            mdd_month = calculate_max_drawdown(window['value'])
            max_drop_week = calculate_consecutive_drops(week['value'])
            tech = calculate_technical_indicators(df.iloc[:i])  # ä½¿ç”¨æˆªè‡³ T æ—¥çš„æ‰€æœ‰æ•°æ®

            if (max_drop_month >= TH["min_consecutive_drop_days"] and
                mdd_month >= TH["min_month_drawdown"]):

                row = {
                    'åŸºé‡‘ä»£ç ': fund_code,
                    'æ—¥æœŸ': df.iloc[i-1]['date'].strftime('%Y-%m-%d'),
                    'æœ€æ–°å‡€å€¼': tech['æœ€æ–°å‡€å€¼'],
                    'æœ€å¤§å›æ’¤': mdd_month,
                    'æœ€å¤§è¿ç»­ä¸‹è·Œ': max_drop_month,
                    'è¿‘ä¸€å‘¨è¿è·Œ': max_drop_week,
                    'RSI': tech['RSI'],
                    'å½“æ—¥è·Œå¹…': tech['å½“æ—¥è·Œå¹…'],
                }
                row['ä¿¡å·'] = generate_signal(pd.Series(row))

                # æœªæ¥æ”¶ç›Š
                future_prices = df.iloc[i: i + max(FORWARD_DAYS) + 1]['value'].values
                base_price = df.iloc[i-1]['value']
                for d in FORWARD_DAYS:
                    if len(future_prices) > d:
                        row[f'æœªæ¥{d}æ—¥æ”¶ç›Š'] = (future_prices[d] - base_price) / base_price
                    else:
                        row[f'æœªæ¥{d}æ—¥æ”¶ç›Š'] = np.nan
                records.append(row)

        return records

    except Exception as e:
        logger.error(f"å›æµ‹ {fund_code} å¤±è´¥: {e}")
        return []


# ================================
# ä¸»å›æµ‹æµç¨‹
# ================================
def run_backtest():
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    yyyymm = now.strftime('%Y%m')
    out_dir = os.path.join(OUTPUT_ROOT, yyyymm)
    os.makedirs(out_dir, exist_ok=True)

    timestamp = now.strftime('%Y%m%d_%H%M%S')
    summary_path = os.path.join(out_dir, f"backtest_summary_{timestamp}.csv")
    report_path = os.path.join(out_dir, f"backtest_report_{timestamp}.md")

    csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    logger.info(f"å‘ç° {len(csv_files)} åªåŸºé‡‘ï¼Œå¼€å§‹å¹¶è¡Œå›æµ‹...")

    all_records = []
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(backtest_single_fund, f) for f in csv_files]
        for future in futures:
            all_records.extend(future.result())

    if not all_records:
        logger.info("æ— ç¬¦åˆæ¡ä»¶çš„å›æµ‹ä¿¡å·")
        return

    df_all = pd.DataFrame(all_records)
    df_all.to_csv(summary_path, index=False, encoding='utf-8-sig')
    logger.info(f"è¯¦ç»†å›æµ‹æ•°æ®å·²ä¿å­˜ï¼š{summary_path}")

    # ================================
    # ç»Ÿè®¡æŠ¥å‘Š
    # ================================
    signal_groups = df_all.groupby('ä¿¡å·')

    report_lines = [
        f"# åŸºé‡‘é¢„è­¦ç­–ç•¥å›æµ‹æŠ¥å‘Š\n",
        f"**ç”Ÿæˆæ—¶é—´**ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)\n",
        f"**å›æµ‹èŒƒå›´**ï¼š{START_DATE or 'å…¨éƒ¨'} ~ {END_DATE or 'å…¨éƒ¨'}\n",
        f"**åŸºé‡‘æ•°é‡**ï¼š{len(set(df_all['åŸºé‡‘ä»£ç ']))}ï¼Œ**ä¿¡å·æ€»æ•°**ï¼š{len(df_all)}\n\n",
        "---\n"
    ]

    for signal_name, group in signal_groups:
        if signal_name == "æ— ä¿¡å·":
            continue
        report_lines.append(f"## {signal_name}\n")
        report_lines.append(f"**å‡ºç°æ¬¡æ•°**ï¼š{len(group)}\n")

        stats = []
        for d in FORWARD_DAYS:
            col = f'æœªæ¥{d}æ—¥æ”¶ç›Š'
            valid = group[col].dropna()
            if len(valid) == 0:
                continue
            win_rate = (valid > 0).mean()
            avg_ret = valid.mean()
            median_ret = valid.median()
            max_ret = valid.max()
            min_ret = valid.min()
            stats.append({
                'æŒæœ‰å¤©æ•°': d,
                'èƒœç‡': win_rate,
                'å¹³å‡æ”¶ç›Š': avg_ret,
                'ä¸­ä½æ•°æ”¶ç›Š': median_ret,
                'æœ€å¤§æ”¶ç›Š': max_ret,
                'æœ€å°æ”¶ç›Š': min_ret,
                'æ ·æœ¬æ•°': len(valid)
            })

        if stats:
            df_stats = pd.DataFrame(stats)
            report_lines.append("\n### æ”¶ç›Šåˆ†å¸ƒ\n")
            report_lines.append("| æŒæœ‰å¤©æ•° | èƒœç‡ | å¹³å‡æ”¶ç›Š | ä¸­ä½æ•° | æœ€å¤§ | æœ€å° | æ ·æœ¬ |\n")
            report_lines.append("| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n")
            for _, r in df_stats.iterrows():
                report_lines.append(
                    f"| {r['æŒæœ‰å¤©æ•°']} | {r['èƒœç‡']:.1%} | {r['å¹³å‡æ”¶ç›Š']:.2%} | "
                    f"{r['ä¸­ä½æ•°æ”¶ç›Š']:.2%} | {r['æœ€å¤§æ”¶ç›Š']:.2%} | {r['æœ€å°æ”¶ç›Š']:.2%} | {r['æ ·æœ¬æ•°']} |\n"
                )
            report_lines.append("\n")

        # åŸºå‡†å¯¹æ¯”ï¼ˆæŒæœ‰20å¤©ï¼‰
        bench_col = f'æœªæ¥{BENCH_DAYS}æ—¥æ”¶ç›Š'
        if bench_col in group.columns:
            bench_valid = group[bench_col].dropna()
            if len(bench_valid) > 0:
                bench_ret = bench_valid.mean()
                report_lines.append(f"**åŸºå‡†æŒæœ‰ {BENCH_DAYS} å¤©å¹³å‡æ”¶ç›Š**ï¼š{bench_ret:.2%}\n\n")
        report_lines.append("---\n")

    with open(report_path, "w", encoding="utf-8") as f:
        f.writelines(report_lines)

    logger.info(f"å›æµ‹æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")


if __name__ == "__main__":
    run_backtest()