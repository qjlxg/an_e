import pandas as pd
import numpy as np
import os
import re
import concurrent.futures
import datetime
import requests
import random
import time
from bs4 import BeautifulSoup
import warnings

# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning)

# --- é…ç½®å‚æ•° ---
FUND_DATA_DIR = 'fund_data'
OUTPUT_FILE = 'fund_analysis_summary_with_info_improved.csv'
MAX_THREADS = 10
TRADING_DAYS_PER_YEAR = 250  # æ¯å¹´å¹³å‡äº¤æ˜“æ—¥æ•°é‡
RISK_FREE_RATE = 0.02  # æ— é£é™©åˆ©ç‡ 2%
ROLLING_PERIODS = {
    '1å‘¨': 5,
    '1æœˆ': 20,
    '1å­£åº¦': 60,
    'åŠå¹´': 120,
    '1å¹´': 250
}
FUND_INFO_CACHE = {}  # ç¼“å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…é‡å¤è¯·æ±‚
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0'
]

# --- è¾…åŠ©å‡½æ•°ï¼šç½‘ç»œè¯·æ±‚ ---

def fetch_fund_info(fund_code):
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ï¼Œä½¿ç”¨ BeautifulSoup å¢å¼ºè§£æé²æ£’æ€§ï¼Œå¹¶åŠ å…¥åçˆ¬æœºåˆ¶ã€‚"""
    if fund_code in FUND_INFO_CACHE:
        return FUND_INFO_CACHE[fund_code]

    url = f'http://fund.eastmoney.com/{fund_code}.html'
    headers = {'User-Agent': random.choice(USER_AGENTS)}

    # å¢åŠ è¯·æ±‚å»¶æ—¶ï¼Œé™ä½è¢«å°ç¦çš„é£é™©
    time.sleep(1) 

    defaults = {
        'name': f'åç§°æŸ¥æ‰¾å¤±è´¥({fund_code})', 
        'size': 'N/A', 
        'type': 'N/A', 
        'daily_growth': 'N/A', 
        'net_value': 'N/A', 
        'rate': 'N/A'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        content = response.text
        
        soup = BeautifulSoup(content, 'html.parser')

        # 1. æå–åŸºé‡‘ç®€ç§°å’Œä»£ç 
        title_tag = soup.find('div', class_='fundDetail-tit')
        if title_tag and title_tag.find('h4'):
            # æå–æ–‡æœ¬ï¼Œå¹¶æ¸…é™¤åŸºé‡‘ä»£ç éƒ¨åˆ†
            full_name = title_tag.find('h4').text.strip()
            defaults['name'] = re.sub(r'\(.*?\)$', '', full_name).strip()
        
        # 2. æå–èµ„äº§è§„æ¨¡ã€åŸºé‡‘ç±»å‹ç­‰ä¿¡æ¯ï¼ˆæ›´ç¨³å¥çš„è¡¨æ ¼è§£æï¼‰
        fund_info_div = soup.find('div', class_='infoOfFund')
        if fund_info_div:
            # æå–åŸºé‡‘ç±»å‹
            type_match = re.search(r'åŸºé‡‘ç±»å‹ï¼š[^<]+<a[^>]+>([\u4e00-\u9fa5]+)</a>', content)
            if type_match:
                defaults['type'] = type_match.group(1).strip()
                
            # æå–èµ„äº§è§„æ¨¡
            size_element = soup.find('th', text=re.compile(r'èµ„äº§è§„æ¨¡'))
            if size_element:
                size_td = size_element.find_next_sibling('td')
                if size_td:
                    defaults['size'] = size_td.text.strip()
        
        # 3. æå–è´¹ç‡
        rate_element = soup.find('th', text=re.compile(r'ç®¡ç†è´¹ç‡'))
        if rate_element:
            rate_td = rate_element.find_next_sibling('td')
            if rate_td:
                defaults['rate'] = rate_td.text.strip()

        # 4. æå–æœ€æ–°å‡€å€¼å’Œæ—¥æ¶¨è·Œå¹…
        data_div = soup.find('dl', class_='dataItem02')
        if data_div:
            # æœ€æ–°å‡€å€¼
            net_value_tag = data_div.find('span', id='gz_nav')
            if net_value_tag:
                defaults['net_value'] = net_value_tag.text.strip()
            
            # æ—¥æ¶¨è·Œå¹…
            daily_growth_tag = data_div.find('span', id='gz_rate')
            if daily_growth_tag:
                defaults['daily_growth'] = daily_growth_tag.text.strip()

    except requests.exceptions.RequestException as e:
        print(f"âŒ åŸºé‡‘ {fund_code} ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ åŸºé‡‘ {fund_code} æ•°æ®è§£æå¤±è´¥: {e}")
    
    FUND_INFO_CACHE[fund_code] = defaults
    return defaults

# --- æ ¸å¿ƒè®¡ç®—å‡½æ•° ---

def calculate_metrics(df, fund_code):
    """è®¡ç®—åŸºé‡‘çš„å„ç§é£é™©æ”¶ç›ŠæŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚"""
    
    # ç»Ÿä¸€åˆ—åä¸ºå°å†™
    df.columns = df.columns.str.lower()
    
    # æ—¥æœŸå’Œç´¯è®¡å‡€å€¼é¢„å¤„ç†
    df = df.rename(columns={'ç´¯è®¡å‡€å€¼': 'cumulative_net_value', 'date': 'date'})
    
    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN
    df['cumulative_net_value'] = pd.to_numeric(df['cumulative_net_value'], errors='coerce')
    
    # ğŸŒŸ å…³é”®ä¿®æ­£ 1: å¼‚å¸¸å€¼ä¿®æ­£ (è§£å†³å¤©æ–‡æ•°å­—æ”¶ç›Šç‡)
    # å°†ç´¯è®¡å‡€å€¼å¤§äº 50 çš„å¼‚å¸¸å€¼è§†ä¸ºå°æ•°ç‚¹é”™ä½ï¼Œå¹¶é™¤ä»¥ 100 ä¿®æ­£
    mask_high_error = df['cumulative_net_value'] > 50 
    if mask_high_error.any():
        print(f"âš ï¸ åŸºé‡‘ {fund_code} å‘ç°å¹¶ä¿®æ­£äº† {mask_high_error.sum()} ä¸ªæç«¯å‡€å€¼å¼‚å¸¸ç‚¹ã€‚")
        # å‡è®¾æ˜¯å°æ•°ç‚¹ç§»åŠ¨ä¸¤ä½ï¼Œè¿›è¡Œä¿®æ­£
        df.loc[mask_high_error, 'cumulative_net_value'] = df.loc[mask_high_error, 'cumulative_net_value'] / 100 
    
    # æ¸…é™¤ NaN å€¼
    df = df.dropna(subset=['cumulative_net_value', 'date'])
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
    try:
        df['date'] = pd.to_datetime(df['date'])
    except:
        # å¦‚æœæ—¥æœŸæ ¼å¼æ··ä¹±ï¼Œå°è¯•æ›´é€šç”¨çš„è§£æ
        df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if pd.notna(x) else np.nan)
        df = df.dropna(subset=['date'])

    # æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values(by='date').reset_index(drop=True)
    
    if len(df) < 2:
        return None, None
        
    cumulative_net_value = df['cumulative_net_value']

    # --- 1. å¹´åŒ–æ”¶ç›Šç‡ (ä¿®æ­£ï¼šä½¿ç”¨å®é™…äº¤æ˜“æ—¥æ•°é‡) ---
    total_return = (cumulative_net_value.iloc[-1] / cumulative_net_value.iloc[0]) - 1
    num_trading_days = len(cumulative_net_value) - 1
    
    if num_trading_days > 0:
        annual_return = (1 + total_return) ** (TRADING_DAYS_PER_YEAR / num_trading_days) - 1
    else:
        annual_return = np.nan

    # --- 2. å¹´åŒ–æ ‡å‡†å·®å’Œæ—¥æ”¶ç›Šç‡ ---
    returns = cumulative_net_value.pct_change().dropna()
    annual_volatility = returns.std() * np.sqrt(TRADING_DAYS_PER_YEAR)
    
    # --- 3. æœ€å¤§å›æ’¤ ---
    max_drawdown = (cumulative_net_value / cumulative_net_value.expanding().max() - 1).min()

    # --- 4. å¤æ™®æ¯”ç‡ ---
    if annual_volatility > 0:
        sharpe_ratio = (annual_return - RISK_FREE_RATE) / annual_volatility
    else:
        sharpe_ratio = np.nan
        
    # --- 5. æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡ ---
    rolling_metrics = {}
    for name, period_days in ROLLING_PERIODS.items():
        if len(returns) >= period_days:
            # è®¡ç®—æ»šåŠ¨æ”¶ç›Šç‡ï¼Œå¹¶å¹´åŒ– (period_days ä¸ºäº¤æ˜“æ—¥)
            rolling_ann_returns = (cumulative_net_value.pct_change(periods=period_days) + 1).pow(TRADING_DAYS_PER_YEAR / period_days) - 1
            # å–å¹³å‡å€¼
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = rolling_ann_returns.mean()
        else:
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = np.nan

    metrics = {
        'åŸºé‡‘ä»£ç ': fund_code,
        'èµ·å§‹æ—¥æœŸ': df['date'].iloc[0].strftime('%Y-%m-%d'),
        'ç»“æŸæ—¥æœŸ': df['date'].iloc[-1].strftime('%Y-%m-%d'),
        'å¹´åŒ–æ”¶ç›Šç‡(æ€»)': annual_return,
        'å¹´åŒ–æ ‡å‡†å·®(æ€»)': annual_volatility,
        'æœ€å¤§å›æ’¤(MDD)': max_drawdown,
        'å¤æ™®æ¯”ç‡(æ€»)': sharpe_ratio,
        **rolling_metrics
    }
    
    return metrics, df['date'].iloc[0], df['date'].iloc[-1]

# --- ä¸»æ‰§è¡Œå‡½æ•° ---

def main():
    if not os.path.isdir(FUND_DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®ç›®å½• '{FUND_DATA_DIR}'ã€‚è¯·åˆ›å»ºæ­¤ç›®å½•å¹¶å°†CSVæ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
        return

    csv_files = [f for f in os.listdir(FUND_DATA_DIR) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ é”™è¯¯ï¼š'{FUND_DATA_DIR}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ã€‚")
        return

    fund_codes = [f.split('.')[0] for f in csv_files]
    all_metrics = []
    
    # é˜¶æ®µ 1: è®¡ç®—æŒ‡æ ‡å¹¶ç¡®å®šå…±åŒåˆ†ææœŸ
    print(f"--- é˜¶æ®µ 1/2: è®¡ç®— {len(fund_codes)} æ”¯åŸºé‡‘çš„é£é™©æ”¶ç›ŠæŒ‡æ ‡ ---")
    start_dates = []
    end_dates = []
    
    for fund_code in fund_codes:
        file_path = os.path.join(FUND_DATA_DIR, f'{fund_code}.csv')
        try:
            # å°è¯•å¤šç§åˆ†éš”ç¬¦è¯»å–
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='gbk')
            except pd.errors.ParserError:
                 df = pd.read_csv(file_path, encoding='utf-8', sep='\t')
            
            metrics, start_date, end_date = calculate_metrics(df.copy(), fund_code)
            
            if metrics:
                all_metrics.append(metrics)
                start_dates.append(start_date)
                end_dates.append(end_date)
        
        except Exception as e:
            print(f"âŒ åŸºé‡‘ {fund_code} å¤„ç†å¤±è´¥: {e}")
            
    if not all_metrics:
        print("æ‰€æœ‰åŸºé‡‘æ•°æ®å¤„ç†å‡å¤±è´¥ã€‚")
        return

    # ç¡®å®šå…±åŒåˆ†ææœŸ
    latest_start = max(start_dates) if start_dates else None
    earliest_end = min(end_dates) if end_dates else None

    # é˜¶æ®µ 2: è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ (å¤šçº¿ç¨‹)
    print(f"\n--- é˜¶æ®µ 2/2: å¤šçº¿ç¨‹è·å– {len(fund_codes)} æ”¯åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ ---")
    fund_codes_to_fetch = [m['åŸºé‡‘ä»£ç '] for m in all_metrics]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # æäº¤æ‰€æœ‰ç½‘ç»œè¯·æ±‚ä»»åŠ¡
        future_to_code = {executor.submit(fetch_fund_info, code): code for code in fund_codes_to_fetch}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_code):
            code = future_to_code[future]
            try:
                # ç»“æœå·²å­˜å…¥å…¨å±€ç¼“å­˜ FUND_INFO_CACHE
                _ = future.result() 
            except Exception as e:
                print(f"âŒ åŸºé‡‘ {code} ä¿¡æ¯è·å–å¤±è´¥: {e}")

    # é˜¶æ®µ 3: æ•´åˆå’Œè¾“å‡º
    print("\n--- é˜¶æ®µ 3/3: æ•´åˆæ•°æ®å¹¶è¾“å‡ºç»“æœ ---")
    final_df = pd.DataFrame(all_metrics)

    # åˆå¹¶åŸºæœ¬ä¿¡æ¯
    info_list = [FUND_INFO_CACHE[code] for code in final_df['åŸºé‡‘ä»£ç ']]
    info_df = pd.DataFrame(info_list).rename(columns={'name': 'åŸºé‡‘ç®€ç§°', 'size': 'èµ„äº§è§„æ¨¡', 'type': 'åŸºé‡‘ç±»å‹', 'daily_growth': 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'net_value': 'æœ€æ–°å‡€å€¼', 'rate': 'ç®¡ç†è´¹ç‡'})
    
    # æ’å…¥ä¿¡æ¯åˆ—åˆ° DataFrame å¤´éƒ¨
    final_df = pd.concat([info_df, final_df], axis=1)
    
    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”å’Œæ•°å­—
    for col in final_df.columns:
        if ('æ”¶ç›Šç‡' in col or 'æ ‡å‡†å·®' in col or 'å›æ’¤' in col) and col != 'å¤æ™®æ¯”ç‡(æ€»)':
            # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å­—ç¬¦ä¸²
            final_df[col] = final_df[col].apply(lambda x: f'{x * 100:.2f}%' if pd.notna(x) else 'N/A')
        elif 'å¤æ™®æ¯”ç‡(æ€»)' in col:
            final_df[col] = final_df[col].apply(lambda x: f'{x:.3f}' if pd.notna(x) else 'N/A')
            # æ·»åŠ ä¸´æ—¶æ•°å­—åˆ—ç”¨äºæ’åº
            final_df['å¤æ™®æ¯”ç‡(æ€»)_Num'] = final_df[col].replace({'N/A': np.nan}).astype(float)
            
    # æ’åºï¼ˆæŒ‰å¤æ™®æ¯”ç‡é™åºï¼‰
    final_df = final_df.sort_values(by='å¤æ™®æ¯”ç‡(æ€»)_Num', ascending=False).drop(columns=['å¤æ™®æ¯”ç‡(æ€»)_Num']).reset_index(drop=True)
    
    # è¾“å‡ºå…±åŒåˆ†ææœŸä¿¡æ¯
    common_period = f'æ‰€æœ‰åŸºé‡‘å…±åŒåˆ†ææœŸï¼š{latest_start.strftime("%Y-%m-%d")} åˆ° {earliest_end.strftime("%Y-%m-%d")}'
    print(common_period)
    
    # å°†å…±åŒåˆ†ææœŸä¿¡æ¯æ·»åŠ åˆ°è¾“å‡ºæ–‡ä»¶çš„ç¬¬ä¸€è¡Œ
    header = pd.DataFrame([{'åŸºé‡‘ä»£ç ': common_period}]).append(final_df.columns.to_series().T, ignore_index=True)
    header.columns = final_df.columns
    final_output = pd.concat([header.iloc[0:1], final_df], ignore_index=True)
    
    final_output.to_csv(OUTPUT_FILE, index=False, encoding='utf_8_sig')
    print(f"\nâœ… æˆåŠŸï¼šåˆ†æç»“æœå·²ä¿å­˜è‡³ {os.path.abspath(OUTPUT_FILE)}")
    
if __name__ == '__main__':
    main()
