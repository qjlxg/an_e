import pandas as pd
import numpy as np
import os
import re
import concurrent.futures
import datetime
import requests
import random
import time
from bs4 import BeautifulSoup
import warnings

# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
try:
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
except AttributeError:
    warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning)

# --- é…ç½®å‚æ•° ---
FUND_DATA_DIR = 'fund_data'
OUTPUT_FILE = 'fund_analysis_summary_optimized.csv' 
MAX_THREADS = 10
TRADING_DAYS_PER_YEAR = 250  # æ¯å¹´å¹³å‡äº¤æ˜“æ—¥æ•°é‡
RISK_FREE_RATE = 0.02  # æ— é£é™©åˆ©ç‡ 2%
EPSILON = 1e-10 # ç”¨äºå‡ ä½•å¹³å‡è®¡ç®—ï¼Œé˜²æ­¢ log(<=0) å¯¼è‡´çš„ RuntimeWarning

ROLLING_PERIODS = {
    '1æœˆ': 20,
    '1å­£åº¦': 60,
    'åŠå¹´': 120,
    '1å¹´': 250
}
FUND_INFO_CACHE = {}  # ç¼“å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0'
]

# --- è¾…åŠ©å‡½æ•°ï¼šç½‘ç»œè¯·æ±‚ (å¢å¼ºé²æ£’æ€§ & ç²¾ç¡®è§£æ) ---
def fetch_fund_info(fund_code):
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¢å¼ºåçˆ¬æœºåˆ¶å’Œè§£æç²¾åº¦ã€‚"""
    if fund_code in FUND_INFO_CACHE:
        return FUND_INFO_CACHE[fund_code]

    # ç›´æ¥è¯·æ±‚ F10 åŸºæœ¬æ¦‚å†µé¡µé¢ï¼Œæ•°æ®æ›´é›†ä¸­
    url = f'http://fundf10.eastmoney.com/jbgk_{fund_code}.html' 
    headers = {'User-Agent': random.choice(USER_AGENTS)}
    
    # ã€ä¼˜åŒ– 1ï¼šå¢åŠ éšæœºç­‰å¾…æ—¶é—´ï¼Œæé«˜æˆåŠŸç‡ã€‘
    time.sleep(random.uniform(2, 4)) 

    defaults = {
        'name': f'åç§°æŸ¥æ‰¾å¤±è´¥({fund_code})', 
        'size': 'N/A', 
        'type': 'N/A', 
        'daily_growth': 'N/A', 
        'net_value': 'N/A', 
        'rate': 'N/A'
    }

    try:
        # ã€ä¼˜åŒ– 2ï¼šå¢åŠ è¶…æ—¶æ—¶é—´ï¼Œåº”å¯¹ç½‘ç»œå»¶è¿Ÿã€‘
        response = requests.get(url, headers=headers, timeout=20) 
        response.raise_for_status()
        content = response.text
        
        # ã€ä¼˜åŒ– 3ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„ BeautifulSoup è§£æã€‘
        soup = BeautifulSoup(content, 'html.parser')
        
        # --- 1. æå–åŸºé‡‘ç®€ç§° ---
        # æŸ¥æ‰¾ .bs_jz ä¸‹çš„ h4.title a æ ‡ç­¾ï¼Œå¹¶ä» title å±æ€§ä¸­æå–
        title_tag = soup.select_one('.basic-new .bs_jz h4.title a')
        if title_tag and 'title' in title_tag.attrs:
            full_name = title_tag['title']
            # å‰¥ç¦»ä»£ç ï¼Œä¿ç•™ç®€ç§°
            defaults['name'] = re.sub(r'\(.*?\)$', '', full_name).strip() 

        # --- 2. æå–æœ€æ–°å‡€å€¼å’Œæ—¥æ¶¨è·Œå¹… ---
        # æŸ¥æ‰¾ .bs_jz .col-right .row1 bï¼Œè¿™ä¸ªæ ‡ç­¾åŒ…å« 'å‡€å€¼ (æ¶¨è·Œå¹…)'
        net_value_tag = soup.select_one('.basic-new .bs_jz .col-right .row1 b')
        if net_value_tag:
            text = net_value_tag.text.strip()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å‡€å€¼å’Œæ¶¨è·Œå¹…
            parts = re.split(r'\s*\((.*?)\)\s*', text, 1) 
            if len(parts) >= 3:
                defaults['net_value'] = parts[0].strip()
                defaults['daily_growth'] = f'({parts[1]})'
            else:
                 defaults['net_value'] = parts[0].strip()
                 
        # --- 3. æå–åŸºé‡‘ç±»å‹å’Œèµ„äº§è§„æ¨¡ (ä» .bs_gl å—) ---
        bs_gl = soup.select_one('.basic-new .bs_gl')
        if bs_gl:
            # æå–ç±»å‹
            type_label = bs_gl.find('label', string=re.compile(r'ç±»å‹ï¼š'))
            if type_label and type_label.find('span'):
                 defaults['type'] = type_label.find('span').text.strip()

            # æå–èµ„äº§è§„æ¨¡
            size_label = bs_gl.find('label', string=re.compile(r'èµ„äº§è§„æ¨¡ï¼š'))
            if size_label and size_label.find('span'):
                defaults['size'] = size_label.find('span').text.strip()


        # --- 4. æå–ç®¡ç†è´¹ç‡ (ä» .info w790 è¡¨æ ¼) ---
        info_table = soup.select_one('table.info.w790')
        if info_table:
            # æŸ¥æ‰¾<th>åŒ…å«'ç®¡ç†è´¹ç‡'çš„è¡Œï¼Œå¹¶è·å–å…¶ä¸‹ä¸€ä¸ª<td>
            rate_th = info_table.find('th', string=re.compile(r'ç®¡ç†è´¹ç‡'))
            if rate_th:
                # å®šä½åˆ°ç®¡ç†è´¹ç‡æ‰€åœ¨çš„ td æ ‡ç­¾
                rate_td = rate_th.find_next_sibling('td')
                if rate_td:
                    defaults['rate'] = rate_td.text.strip()
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ åŸºé‡‘ {fund_code} ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ åŸºé‡‘ {fund_code} æ•°æ®è§£æå¤±è´¥: {e}")
    
    FUND_INFO_CACHE[fund_code] = defaults
    return defaults


# --- æ ¸å¿ƒè®¡ç®—å‡½æ•° (å¢å¼ºæ•°æ®æ¸…æ´—) ---

def calculate_metrics(df, fund_code):
    """è®¡ç®—åŸºé‡‘çš„å„ç§é£é™©æ”¶ç›ŠæŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œæ•°æ®æ¸…æ´—å’Œä¼˜åŒ–ã€‚"""
    global EPSILON
    
    df.columns = df.columns.str.lower()
    df = df.rename(columns={'ç´¯è®¡å‡€å€¼': 'cumulative_net_value', 'date': 'date'})
    df['cumulative_net_value'] = pd.to_numeric(df['cumulative_net_value'], errors='coerce')
    
    # ã€æ•°æ®æ¸…æ´— 1ï¼šæç«¯å¼‚å¸¸å€¼ä¿®æ­£ (é’ˆå¯¹å¯èƒ½çš„è¾“å…¥é”™è¯¯)ã€‘
    mask_high_error = df['cumulative_net_value'] > 50 
    if mask_high_error.any():
        print(f"âš ï¸ åŸºé‡‘ {fund_code} å‘ç°å¹¶ä¿®æ­£äº† {mask_high_error.sum()} ä¸ªæç«¯å‡€å€¼å¼‚å¸¸ç‚¹ï¼ˆ>50ï¼‰ã€‚")
        df.loc[mask_high_error, 'cumulative_net_value'] = df.loc[mask_high_error, 'cumulative_net_value'] / 100 
    
    df = df.dropna(subset=['cumulative_net_value', 'date'])

    # ã€æ•°æ®æ¸…æ´— 2ï¼šè§£å†³ -100% MDD å’Œ inf% æ”¶ç›Šç‡çš„å…³é”®ä¿®æ­£ã€‘
    # å°†å‡€å€¼ä¸­å°äºæˆ–ç­‰äº0çš„å€¼è®¾ä¸º NaNï¼Œç„¶ååˆ é™¤è¯¥è¡Œ
    mask_zero_or_negative = df['cumulative_net_value'] <= 0
    if mask_zero_or_negative.any():
        print(f"ğŸ’£ åŸºé‡‘ {fund_code} å‘ç° {mask_zero_or_negative.sum()} ä¸ªé›¶æˆ–è´Ÿå‡€å€¼ï¼Œå·²ç§»é™¤ä»¥ç¡®ä¿æŒ‡æ ‡è®¡ç®—æœ‰æ•ˆã€‚")
        df.loc[mask_zero_or_negative, 'cumulative_net_value'] = np.nan
        df = df.dropna(subset=['cumulative_net_value'])
    
    try:
        df.loc[:, 'date'] = pd.to_datetime(df['date'])
    except:
        df.loc[:, 'date'] = df['date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if pd.notna(x) else np.nan)
        df = df.dropna(subset=['date'])

    df = df.sort_values(by='date').reset_index(drop=True)
    
    if len(df) < 2:
        return None, None, None
        
    cumulative_net_value = df['cumulative_net_value']

    # --- 1. å¹´åŒ–æ”¶ç›Šç‡ (åŸºäºäº¤æ˜“æ—¥) ---
    if cumulative_net_value.iloc[0] <= 0:
        annual_return = np.nan
    else:
        total_return = (cumulative_net_value.iloc[-1] / cumulative_net_value.iloc[0]) - 1
        num_trading_days = len(cumulative_net_value) - 1
        
        if num_trading_days > 0:
            # å‡ ä½•å¹³å‡å¹´åŒ–
            annual_return = (1 + total_return) ** (TRADING_DAYS_PER_YEAR / num_trading_days) - 1
        else:
            annual_return = np.nan

    # --- 2. å¹´åŒ–æ ‡å‡†å·®å’Œæ—¥æ”¶ç›Šç‡ ---
    returns = cumulative_net_value.pct_change().dropna()
    
    annual_volatility = returns.std() * np.sqrt(TRADING_DAYS_PER_YEAR)
    
    # --- 3. æœ€å¤§å›æ’¤ ---
    max_drawdown = (cumulative_net_value / cumulative_net_value.expanding().max() - 1).min()

    # --- 4. å¤æ™®æ¯”ç‡ ---
    if annual_volatility > EPSILON: # é¿å…é™¤ä»¥é›¶
        sharpe_ratio = (annual_return - RISK_FREE_RATE) / annual_volatility
    else:
        sharpe_ratio = np.nan
        
    # --- 5. æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡ (ä½¿ç”¨å‡ ä½•å¹³å‡å¹³æ»‘å¼‚å¸¸å€¼) ---
    rolling_metrics = {}
    
    for name, period_days in ROLLING_PERIODS.items():
        if len(cumulative_net_value) >= period_days:
            # 1. è®¡ç®—æ‰€æœ‰éå¹´åŒ–çš„æœŸé—´æ”¶ç›Šç‡ (R_p)
            rolling_non_ann_returns = cumulative_net_value.pct_change(periods=period_days).dropna()
            
            # 2. å°†æ”¶ç›Šç‡è½¬æ¢ä¸º (1 + R_p)
            compounding_factors = 1 + rolling_non_ann_returns
            
            # ã€ä¼˜åŒ–ï¼šä½¿ç”¨ EPSILON é¿å… log(<=0) å¯¼è‡´çš„ RuntimeWarning / infã€‘
            compounding_factors = np.maximum(compounding_factors, EPSILON)

            # 3. è®¡ç®—æ‰€æœ‰å‘¨æœŸæ”¶ç›Šç‡çš„å‡ ä½•å¹³å‡
            log_returns = np.log(compounding_factors)
            mean_log_return = log_returns.mean()
            R_geo = np.exp(mean_log_return) - 1
            
            # 4. å°†å¹³å‡å‡ ä½•æ”¶ç›Šç‡å¹´åŒ–
            annualized_R_geo = (1 + R_geo) ** (TRADING_DAYS_PER_YEAR / period_days) - 1
            
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = annualized_R_geo
        else:
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = np.nan

    metrics = {
        'åŸºé‡‘ä»£ç ': fund_code,
        'èµ·å§‹æ—¥æœŸ': df['date'].iloc[0].strftime('%Y-%m-%d'),
        'ç»“æŸæ—¥æœŸ': df['date'].iloc[-1].strftime('%Y-%m-%d'),
        'å¹´åŒ–æ”¶ç›Šç‡(æ€»)': annual_return,
        'å¹´åŒ–æ ‡å‡†å·®(æ€»)': annual_volatility,
        'æœ€å¤§å›æ’¤(MDD)': max_drawdown,
        'å¤æ™®æ¯”ç‡(æ€»)': sharpe_ratio,
        **rolling_metrics
    }
    
    return metrics, df['date'].iloc[0], df['date'].iloc[-1]

# --- ä¸»æ‰§è¡Œå‡½æ•° ---

def main():
    if not os.path.isdir(FUND_DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®ç›®å½• '{FUND_DATA_DIR}'ã€‚è¯·åˆ›å»ºæ­¤ç›®å½•å¹¶å°†CSVæ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
        return

    csv_files = [f for f in os.listdir(FUND_DATA_DIR) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ é”™è¯¯ï¼š'{FUND_DATA_DIR}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ã€‚")
        return

    fund_codes = [f.split('.')[0] for f in csv_files]
    all_metrics = []
    
    # é˜¶æ®µ 1: è®¡ç®—æŒ‡æ ‡å¹¶ç¡®å®šå…±åŒåˆ†ææœŸ
    print(f"--- é˜¶æ®µ 1/2: è®¡ç®— {len(fund_codes)} æ”¯åŸºé‡‘çš„é£é™©æ”¶ç›ŠæŒ‡æ ‡ ---")
    start_dates = []
    end_dates = []
    
    for fund_code in fund_codes:
        file_path = os.path.join(FUND_DATA_DIR, f'{fund_code}.csv')
        try:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='gbk')
            except pd.errors.ParserError:
                 df = pd.read_csv(file_path, encoding='utf-8', sep='\t')
            
            metrics, start_date, end_date = calculate_metrics(df.copy(), fund_code)
            
            if metrics:
                all_metrics.append(metrics)
                if start_date:
                    start_dates.append(start_date)
                if end_date:
                    end_dates.append(end_date)
        
        except Exception as e:
            print(f"âŒ åŸºé‡‘ {fund_code} å¤„ç†å¤±è´¥: {e}")
            
    if not all_metrics:
        print("æ‰€æœ‰åŸºé‡‘æ•°æ®å¤„ç†å‡å¤±è´¥ã€‚")
        return

    # ç¡®å®šå…±åŒåˆ†ææœŸ
    latest_start = max(start_dates) if start_dates else None
    earliest_end = min(end_dates) if end_dates else None

    # é˜¶æ®µ 2: è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ (å¤šçº¿ç¨‹)
    # ç¡®ä¿åªå¯¹æˆåŠŸè®¡ç®—æŒ‡æ ‡çš„åŸºé‡‘è·å–ä¿¡æ¯
    fund_codes_to_fetch = [m['åŸºé‡‘ä»£ç '] for m in all_metrics]
    print(f"\n--- é˜¶æ®µ 2/2: å¤šçº¿ç¨‹è·å– {len(fund_codes_to_fetch)} æ”¯åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ ---")
    
    # ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿä¿¡æ¯æŠ“å–
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_code = {executor.submit(fetch_fund_info, code): code for code in fund_codes_to_fetch}
        
        # å®æ—¶æ‰“å°ä¿¡æ¯ï¼Œå¹¶ç­‰å¾…æ‰€æœ‰æŠ“å–ä»»åŠ¡å®Œæˆ
        _ = [future.result() for future in concurrent.futures.as_completed(future_to_code)]

    # é˜¶æ®µ 3: æ•´åˆå’Œè¾“å‡º
    print("\n--- é˜¶æ®µ 3/3: æ•´åˆæ•°æ®å¹¶è¾“å‡ºç»“æœ ---")
    final_df = pd.DataFrame(all_metrics)

    # ä»ç¼“å­˜ä¸­è·å–åŸºé‡‘ä¿¡æ¯å¹¶æ•´åˆ
    info_list = [FUND_INFO_CACHE[code] for code in final_df['åŸºé‡‘ä»£ç ']]
    info_df = pd.DataFrame(info_list).rename(columns={'name': 'åŸºé‡‘ç®€ç§°', 'size': 'èµ„äº§è§„æ¨¡', 'type': 'åŸºé‡‘ç±»å‹', 'daily_growth': 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'net_value': 'æœ€æ–°å‡€å€¼', 'rate': 'ç®¡ç†è´¹ç‡'})
    
    # é‡ç½®ç´¢å¼•ä»¥ç¡®ä¿æ‹¼æ¥å¯¹é½
    info_df.index = final_df.index
    final_df = pd.concat([info_df, final_df], axis=1)
    
    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”å’Œæ•°å­—
    for col in final_df.columns:
        if ('æ”¶ç›Šç‡' in col or 'æ ‡å‡†å·®' in col or 'å›æ’¤' in col) and col != 'å¤æ™®æ¯”ç‡(æ€»)':
            # ä½¿ç”¨ try/except æ•è· inf å¼‚å¸¸å¹¶å¤„ç†ä¸º N/A
            def format_pct(x):
                if pd.isna(x) or np.isinf(x):
                    return 'N/A'
                return f'{x * 100:.2f}%'
            final_df[col] = final_df[col].apply(format_pct)
        elif 'å¤æ™®æ¯”ç‡(æ€»)' in col:
            def format_sharpe(x):
                if pd.isna(x) or np.isinf(x):
                    return 'N/A'
                return f'{x:.3f}'
            final_df[col] = final_df[col].apply(format_sharpe)
            # åˆ›å»ºä¸€ä¸ªç”¨äºæ’åºçš„ä¸´æ—¶æ•°å­—åˆ—
            final_df['å¤æ™®æ¯”ç‡(æ€»)_Num'] = final_df['å¤æ™®æ¯”ç‡(æ€»)'].replace({'N/A': np.nan}).astype(float)
            
    # æ’åºï¼ˆæŒ‰å¤æ™®æ¯”ç‡é™åºï¼‰
    final_df = final_df.sort_values(by='å¤æ™®æ¯”ç‡(æ€»)_Num', ascending=False).drop(columns=['å¤æ™®æ¯”ç‡(æ€»)_Num']).reset_index(drop=True)
    
    # è¾“å‡ºå…±åŒåˆ†ææœŸä¿¡æ¯
    if latest_start and earliest_end:
        common_period = f'æ‰€æœ‰åŸºé‡‘å…±åŒåˆ†ææœŸï¼š{latest_start.strftime("%Y-%m-%d")} åˆ° {earliest_end.strftime("%Y-%m-%d")}'
        print(common_period)
        
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å…±åŒåˆ†ææœŸä¿¡æ¯çš„æ–°è¡Œ
        period_info_row = pd.Series(
            {'åŸºé‡‘ç®€ç§°': common_period, 'åŸºé‡‘ä»£ç ': 'æ‰€æœ‰åŸºé‡‘å…±åŒåˆ†ææœŸ'},
            index=final_df.columns
        ).to_frame().T
        
        final_output = pd.concat([period_info_row, final_df], ignore_index=True)
    else:
        final_output = final_df
        print("æœªç¡®å®šæœ‰æ•ˆçš„å…±åŒåˆ†ææœŸã€‚")

    
    # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
    target_columns = [
        'åŸºé‡‘ç®€ç§°', 'èµ„äº§è§„æ¨¡', 'åŸºé‡‘ç±»å‹', 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'æœ€æ–°å‡€å€¼', 
        'ç®¡ç†è´¹ç‡', 'åŸºé‡‘ä»£ç ', 'èµ·å§‹æ—¥æœŸ', 'ç»“æŸæ—¥æœŸ', 'å¹´åŒ–æ”¶ç›Šç‡(æ€»)', 
        'å¹´åŒ–æ ‡å‡†å·®(æ€»)', 'æœ€å¤§å›æ’¤(MDD)', 'å¤æ™®æ¯”ç‡(æ€»)', 
        'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1æœˆ)', 'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1å­£åº¦)', 
        'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(åŠå¹´)', 'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1å¹´)'
    ]
    
    # é‡æ–°æ’åˆ—åˆ—ï¼Œå¦‚æœæŸä¸ªåˆ—ç¼ºå¤±åˆ™å¿½ç•¥
    final_output = final_output[[col for col in target_columns if col in final_output.columns]]


    # ä½¿ç”¨ utf_8_sig ç¼–ç ä»¥ç¡®ä¿ Excel ä¸­æ–‡ä¸ä¹±ç 
    final_output.to_csv(OUTPUT_FILE, index=False, encoding='utf_8_sig')
    print(f"\nâœ… æˆåŠŸï¼šåˆ†æç»“æœå·²ä¿å­˜è‡³ {os.path.abspath(OUTPUT_FILE)}")
    
if __name__ == '__main__':
    main()
