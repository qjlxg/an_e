import pandas as pd
import numpy as np
import os
import re
import concurrent.futures
import datetime
import requests
import random
import time
from bs4 import BeautifulSoup
import warnings

# --- æ ¸å¿ƒä¿®æ­£ï¼šå°† SettingWithCopyWarning å¯¼å…¥è·¯å¾„ä» core.common æ›´æ”¹ä¸º errors ---
# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
try:
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
except AttributeError:
    # å…¼å®¹éå¸¸æ—§çš„ Pandas ç‰ˆæœ¬
    warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning)

# --- é…ç½®å‚æ•° ---
FUND_DATA_DIR = 'fund_data'
OUTPUT_FILE = 'fund_analysis_summary_optimized.csv' 
MAX_THREADS = 10
TRADING_DAYS_PER_YEAR = 250  # æ¯å¹´å¹³å‡äº¤æ˜“æ—¥æ•°é‡
RISK_FREE_RATE = 0.02  # æ— é£é™©åˆ©ç‡ 2%

# ã€æ ¸å¿ƒä¿®æ­£ï¼šå–æ¶ˆ 1 å‘¨å‘¨æœŸã€‘
ROLLING_PERIODS = {
    '1æœˆ': 20,
    '1å­£åº¦': 60,
    'åŠå¹´': 120,
    '1å¹´': 250
}
FUND_INFO_CACHE = {}  # ç¼“å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0'
]

# --- è¾…åŠ©å‡½æ•°ï¼šç½‘ç»œè¯·æ±‚ (ä¿æŒä¸å˜) ---
def fetch_fund_info(fund_code):
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ï¼Œä½¿ç”¨ BeautifulSoup å¢å¼ºè§£æé²æ£’æ€§ï¼Œå¹¶åŠ å…¥åçˆ¬æœºåˆ¶ã€‚"""
    if fund_code in FUND_INFO_CACHE:
        return FUND_INFO_CACHE[fund_code]

    url = f'http://fund.eastmoney.com/{fund_code}.html'
    headers = {'User-Agent': random.choice(USER_AGENTS)}
    time.sleep(1) 

    defaults = {
        'name': f'åç§°æŸ¥æ‰¾å¤±è´¥({fund_code})', 
        'size': 'N/A', 
        'type': 'N/A', 
        'daily_growth': 'N/A', 
        'net_value': 'N/A', 
        'rate': 'N/A'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        content = response.text
        
        soup = BeautifulSoup(content, 'html.parser')

        # 1. æå–åŸºé‡‘ç®€ç§°å’Œä»£ç 
        title_tag = soup.find('div', class_='fundDetail-tit')
        if title_tag and title_tag.find('h4'):
            full_name = title_tag.find('h4').text.strip()
            defaults['name'] = re.sub(r'\(.*?\)$', '', full_name).strip()
        
        # 2. æå–èµ„äº§è§„æ¨¡ã€åŸºé‡‘ç±»å‹ç­‰ä¿¡æ¯
        size_element = soup.find('th', text=re.compile(r'èµ„äº§è§„æ¨¡'))
        if size_element:
            size_td = size_element.find_next_sibling('td')
            if size_td:
                defaults['size'] = size_td.text.strip()
        
        rate_element = soup.find('th', text=re.compile(r'ç®¡ç†è´¹ç‡'))
        if rate_element:
            rate_td = rate_element.find_next_sibling('td')
            if rate_td:
                defaults['rate'] = rate_td.text.strip()

        # 3. æå–æœ€æ–°å‡€å€¼å’Œæ—¥æ¶¨è·Œå¹…
        data_div = soup.find('dl', class_='dataItem02')
        if data_div:
            net_value_tag = data_div.find('span', id='gz_nav')
            if net_value_tag:
                defaults['net_value'] = net_value_tag.text.strip()
            
            daily_growth_tag = data_div.find('span', id='gz_rate')
            if daily_growth_tag:
                defaults['daily_growth'] = daily_growth_tag.text.strip()

    except requests.exceptions.RequestException as e:
        print(f"âŒ åŸºé‡‘ {fund_code} ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ åŸºé‡‘ {fund_code} æ•°æ®è§£æå¤±è´¥: {e}")
    
    FUND_INFO_CACHE[fund_code] = defaults
    return defaults


# --- æ ¸å¿ƒè®¡ç®—å‡½æ•° (ä»…ä¿®æ”¹æ»šåŠ¨æ”¶ç›Šç‡éƒ¨åˆ†) ---

def calculate_metrics(df, fund_code):
    """è®¡ç®—åŸºé‡‘çš„å„ç§é£é™©æ”¶ç›ŠæŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œæ•°æ®æ¸…æ´—å’Œä¼˜åŒ–ã€‚"""
    
    df.columns = df.columns.str.lower()
    df = df.rename(columns={'ç´¯è®¡å‡€å€¼': 'cumulative_net_value', 'date': 'date'})
    df['cumulative_net_value'] = pd.to_numeric(df['cumulative_net_value'], errors='coerce')
    
    # ã€æ•°æ®æ¸…æ´—ï¼šæç«¯å¼‚å¸¸å€¼ä¿®æ­£ã€‘
    mask_high_error = df['cumulative_net_value'] > 50 
    if mask_high_error.any():
        print(f"âš ï¸ åŸºé‡‘ {fund_code} å‘ç°å¹¶ä¿®æ­£äº† {mask_high_error.sum()} ä¸ªæç«¯å‡€å€¼å¼‚å¸¸ç‚¹ï¼ˆ>50ï¼‰ã€‚")
        # ä½¿ç”¨ .loc è¿›è¡Œèµ‹å€¼ä»¥é¿å… SettingWithCopyWarning
        df.loc[mask_high_error, 'cumulative_net_value'] = df.loc[mask_high_error, 'cumulative_net_value'] / 100 
    
    df = df.dropna(subset=['cumulative_net_value', 'date'])
    
    try:
        df['date'] = pd.to_datetime(df['date'])
    except:
        df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if pd.notna(x) else np.nan)
        df = df.dropna(subset=['date'])

    df = df.sort_values(by='date').reset_index(drop=True)
    
    if len(df) < 2:
        return None, None
        
    cumulative_net_value = df['cumulative_net_value']

    # --- 1. å¹´åŒ–æ”¶ç›Šç‡ (åŸºäºäº¤æ˜“æ—¥) ---
    total_return = (cumulative_net_value.iloc[-1] / cumulative_net_value.iloc[0]) - 1
    num_trading_days = len(cumulative_net_value) - 1
    
    if num_trading_days > 0:
        annual_return = (1 + total_return) ** (TRADING_DAYS_PER_YEAR / num_trading_days) - 1
    else:
        annual_return = np.nan

    # --- 2. å¹´åŒ–æ ‡å‡†å·®å’Œæ—¥æ”¶ç›Šç‡ ---
    returns = cumulative_net_value.pct_change().dropna()
    
    # ã€æ–°åŠ å¼‚å¸¸æ£€æµ‹å’Œè­¦å‘Šã€‘
    mask_extreme_return = returns.abs() > 0.20 # æ¯æ—¥æ”¶ç›Šç‡è¶…è¿‡ 20%
    if mask_extreme_return.any():
        print(f"ğŸ“¢ åŸºé‡‘ {fund_code} è­¦å‘Šï¼šå‘ç° {mask_extreme_return.sum()} ä¸ªæç«¯æ—¥æ”¶ç›Šç‡ï¼ˆ>20%ï¼‰ï¼Œå¯èƒ½å½±å“æ³¢åŠ¨ç‡è®¡ç®—ã€‚")
    
    annual_volatility = returns.std() * np.sqrt(TRADING_DAYS_PER_YEAR)
    
    # --- 3. æœ€å¤§å›æ’¤ ---
    max_drawdown = (cumulative_net_value / cumulative_net_value.expanding().max() - 1).min()

    # --- 4. å¤æ™®æ¯”ç‡ ---
    if annual_volatility > 0:
        sharpe_ratio = (annual_return - RISK_FREE_RATE) / annual_volatility
    else:
        sharpe_ratio = np.nan
        
    # --- 5. æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡ (ä½¿ç”¨å‡ ä½•å¹³å‡å¹³æ»‘å¼‚å¸¸å€¼) ---
    rolling_metrics = {}
    
    for name, period_days in ROLLING_PERIODS.items():
        if len(cumulative_net_value) >= period_days:
            # 1. è®¡ç®—æ‰€æœ‰éå¹´åŒ–çš„æœŸé—´æ”¶ç›Šç‡ (R_p)
            rolling_non_ann_returns = cumulative_net_value.pct_change(periods=period_days).dropna()
            
            # 2. å°†æ”¶ç›Šç‡è½¬æ¢ä¸º (1 + R_p)
            compounding_factors = 1 + rolling_non_ann_returns

            # 3. è®¡ç®—æ‰€æœ‰å‘¨æœŸæ”¶ç›Šç‡çš„å‡ ä½•å¹³å‡
            log_returns = np.log(compounding_factors)
            mean_log_return = log_returns.mean()
            R_geo = np.exp(mean_log_return) - 1
            
            # 4. å°†å¹³å‡å‡ ä½•æ”¶ç›Šç‡å¹´åŒ–
            annualized_R_geo = (1 + R_geo) ** (TRADING_DAYS_PER_YEAR / period_days) - 1
            
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = annualized_R_geo
        else:
            rolling_metrics[f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = np.nan

    metrics = {
        'åŸºé‡‘ä»£ç ': fund_code,
        'èµ·å§‹æ—¥æœŸ': df['date'].iloc[0].strftime('%Y-%m-%d'),
        'ç»“æŸæ—¥æœŸ': df['date'].iloc[-1].strftime('%Y-%m-%d'),
        'å¹´åŒ–æ”¶ç›Šç‡(æ€»)': annual_return,
        'å¹´åŒ–æ ‡å‡†å·®(æ€»)': annual_volatility,
        'æœ€å¤§å›æ’¤(MDD)': max_drawdown,
        'å¤æ™®æ¯”ç‡(æ€»)': sharpe_ratio,
        **rolling_metrics
    }
    
    return metrics, df['date'].iloc[0], df['date'].iloc[-1]

# --- ä¸»æ‰§è¡Œå‡½æ•° (ä¿æŒä¸å˜) ---

def main():
    if not os.path.isdir(FUND_DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®ç›®å½• '{FUND_DATA_DIR}'ã€‚è¯·åˆ›å»ºæ­¤ç›®å½•å¹¶å°†CSVæ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
        return

    csv_files = [f for f in os.listdir(FUND_DATA_DIR) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ é”™è¯¯ï¼š'{FUND_DATA_DIR}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ã€‚")
        return

    fund_codes = [f.split('.')[0] for f in csv_files]
    all_metrics = []
    
    # é˜¶æ®µ 1: è®¡ç®—æŒ‡æ ‡å¹¶ç¡®å®šå…±åŒåˆ†ææœŸ
    print(f"--- é˜¶æ®µ 1/2: è®¡ç®— {len(fund_codes)} æ”¯åŸºé‡‘çš„é£é™©æ”¶ç›ŠæŒ‡æ ‡ ---")
    start_dates = []
    end_dates = []
    
    for fund_code in fund_codes:
        file_path = os.path.join(FUND_DATA_DIR, f'{fund_code}.csv')
        try:
            # å°è¯•å¤šç§åˆ†éš”ç¬¦è¯»å–
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='gbk')
            except pd.errors.ParserError:
                 df = pd.read_csv(file_path, encoding='utf-8', sep='\t')
            
            metrics, start_date, end_date = calculate_metrics(df.copy(), fund_code)
            
            if metrics:
                all_metrics.append(metrics)
                start_dates.append(start_date)
                end_dates.append(end_date)
        
        except Exception as e:
            print(f"âŒ åŸºé‡‘ {fund_code} å¤„ç†å¤±è´¥: {e}")
            
    if not all_metrics:
        print("æ‰€æœ‰åŸºé‡‘æ•°æ®å¤„ç†å‡å¤±è´¥ã€‚")
        return

    # ç¡®å®šå…±åŒåˆ†ææœŸ
    latest_start = max(start_dates) if start_dates else None
    earliest_end = min(end_dates) if end_dates else None

    # é˜¶æ®µ 2: è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ (å¤šçº¿ç¨‹)
    print(f"\n--- é˜¶æ®µ 2/2: å¤šçº¿ç¨‹è·å– {len(fund_codes)} æ”¯åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ ---")
    fund_codes_to_fetch = [m['åŸºé‡‘ä»£ç '] for m in all_metrics]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_code = {executor.submit(fetch_fund_info, code): code for code in fund_codes_to_fetch}
        
        for future in concurrent.futures.as_completed(future_to_code):
            code = future_to_code[future]
            try:
                _ = future.result() 
            except Exception as e:
                print(f"âŒ åŸºé‡‘ {code} ä¿¡æ¯è·å–å¤±è´¥: {e}")

    # é˜¶æ®µ 3: æ•´åˆå’Œè¾“å‡º
    print("\n--- é˜¶æ®µ 3/3: æ•´åˆæ•°æ®å¹¶è¾“å‡ºç»“æœ ---")
    final_df = pd.DataFrame(all_metrics)

    info_list = [FUND_INFO_CACHE[code] for code in final_df['åŸºé‡‘ä»£ç ']]
    info_df = pd.DataFrame(info_list).rename(columns={'name': 'åŸºé‡‘ç®€ç§°', 'size': 'èµ„äº§è§„æ¨¡', 'type': 'åŸºé‡‘ç±»å‹', 'daily_growth': 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'net_value': 'æœ€æ–°å‡€å€¼', 'rate': 'ç®¡ç†è´¹ç‡'})
    
    final_df = pd.concat([info_df, final_df], axis=1)
    
    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”å’Œæ•°å­—
    for col in final_df.columns:
        if ('æ”¶ç›Šç‡' in col or 'æ ‡å‡†å·®' in col or 'å›æ’¤' in col) and col != 'å¤æ™®æ¯”ç‡(æ€»)':
            final_df[col] = final_df[col].apply(lambda x: f'{x * 100:.2f}%' if pd.notna(x) else 'N/A')
        elif 'å¤æ™®æ¯”ç‡(æ€»)' in col:
            final_df[col] = final_df[col].apply(lambda x: f'{x:.3f}' if pd.notna(x) else 'N/A')
            final_df['å¤æ™®æ¯”ç‡(æ€»)_Num'] = final_df[col].replace({'N/A': np.nan}).astype(float)
            
    # æ’åºï¼ˆæŒ‰å¤æ™®æ¯”ç‡é™åºï¼‰
    final_df = final_df.sort_values(by='å¤æ™®æ¯”ç‡(æ€»)_Num', ascending=False).drop(columns=['å¤æ™®æ¯”ç‡(æ€»)_Num']).reset_index(drop=True)
    
    # è¾“å‡ºå…±åŒåˆ†ææœŸä¿¡æ¯
    common_period = f'æ‰€æœ‰åŸºé‡‘å…±åŒåˆ†ææœŸï¼š{latest_start.strftime("%Y-%m-%d")} åˆ° {earliest_end.strftime("%Y-%m-%d")}'
    print(common_period)
    
    # æ·»åŠ å…±åŒåˆ†ææœŸä¿¡æ¯
    # ä¿®æ­£ï¼šç›´æ¥åˆ›å»ºä¸€è¡Œæè¿°ï¼Œé¿å…å¤æ‚çš„appendæ“ä½œ
    period_info_row = pd.Series(
        {'åŸºé‡‘ä»£ç ': common_period},
        index=final_df.columns
    ).to_frame().T
    
    final_output = pd.concat([period_info_row, final_df], ignore_index=True)
    
    final_output.to_csv(OUTPUT_FILE, index=False, encoding='utf_8_sig')
    print(f"\nâœ… æˆåŠŸï¼šåˆ†æç»“æœå·²ä¿å­˜è‡³ {os.path.abspath(OUTPUT_FILE)}")
    
if __name__ == '__main__':
    # å¿…é¡»åœ¨ä¸»å‡½æ•°å¤–è°ƒç”¨ï¼Œæ‰èƒ½åœ¨ç¨‹åºå¯åŠ¨æ—¶ç”Ÿæ•ˆ
    # warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning) 
    main()
