import pandas as pd
import numpy as np
import os
import re
import concurrent.futures
import datetime
import requests
import random
import time
from bs4 import BeautifulSoup
import warnings

# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
try:
    warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
except AttributeError:
    warnings.filterwarnings('ignore', category=pd.core.common.SettingWithCopyWarning)

# --- é…ç½®å‚æ•° ---
FUND_DATA_DIR = 'fund_data'
OUTPUT_FILE = 'fund_analysis_summary_common_period.csv' 
MAX_THREADS = 10
TRADING_DAYS_PER_YEAR = 250  # æ¯å¹´å¹³å‡äº¤æ˜“æ—¥æ•°é‡
RISK_FREE_RATE = 0.02  # æ— é£é™©åˆ©ç‡ 2%
EPSILON = 1e-10 # ç”¨äºå‡ ä½•å¹³å‡è®¡ç®—ï¼Œé˜²æ­¢ log(<=0) å¯¼è‡´çš„ RuntimeWarning

ROLLING_PERIODS = {
    '1æœˆ': 20,
    '1å­£åº¦': 60,
    'åŠå¹´': 120,
    '1å¹´': 250
}
FUND_INFO_CACHE = {}  # ç¼“å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0'
]

# --- è¾…åŠ©å‡½æ•°ï¼šç½‘ç»œè¯·æ±‚ ---
def fetch_fund_info(fund_code):
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ã€‚"""
    if fund_code in FUND_INFO_CACHE:
        return FUND_INFO_CACHE[fund_code]

    url = f'http://fundf10.eastmoney.com/jbgk_{fund_code}.html' 
    headers = {'User-Agent': random.choice(USER_AGENTS)}
    
    time.sleep(random.uniform(2, 4)) 

    defaults = {
        'name': f'åç§°æŸ¥æ‰¾å¤±è´¥({fund_code})', 
        'size': 'N/A', 
        'type': 'N/A', 
        'daily_growth': 'N/A', 
        'net_value': 'N/A', 
        'rate': 'N/A'
    }

    try:
        response = requests.get(url, headers=headers, timeout=20) 
        response.raise_for_status()
        content = response.text
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # --- 1. æå–åŸºé‡‘ç®€ç§° ---
        title_tag = soup.select_one('.basic-new .bs_jz h4.title a')
        if title_tag and 'title' in title_tag.attrs:
            full_name = title_tag['title']
            defaults['name'] = re.sub(r'\(.*?\)$', '', full_name).strip() 

        # --- 2. æå–æœ€æ–°å‡€å€¼å’Œæ—¥æ¶¨è·Œå¹… ---
        net_value_tag = soup.select_one('.basic-new .bs_jz .col-right .row1 b')
        if net_value_tag:
            text = net_value_tag.text.strip()
            parts = re.split(r'\s*\((.*?)\)\s*', text, 1) 
            if len(parts) >= 3:
                defaults['net_value'] = parts[0].strip()
                defaults['daily_growth'] = f'({parts[1]})'
            else:
                 defaults['net_value'] = parts[0].strip()
                 
        # --- 3. æå–åŸºé‡‘ç±»å‹å’Œèµ„äº§è§„æ¨¡ (ä» .bs_gl å—) ---
        bs_gl = soup.select_one('.basic-new .bs_gl')
        if bs_gl:
            type_label = bs_gl.find('label', string=re.compile(r'ç±»å‹ï¼š'))
            if type_label and type_label.find('span'):
                 defaults['type'] = type_label.find('span').text.strip()

            size_label = bs_gl.find('label', string=re.compile(r'èµ„äº§è§„æ¨¡ï¼š'))
            if size_label and size_label.find('span'):
                defaults['size'] = size_label.find('span').text.strip()

        # --- 4. æå–ç®¡ç†è´¹ç‡ (ä» .info w790 è¡¨æ ¼) ---
        info_table = soup.select_one('table.info.w790')
        if info_table:
            rate_th = info_table.find('th', string=re.compile(r'ç®¡ç†è´¹ç‡'))
            if rate_th:
                rate_td = rate_th.find_next_sibling('td')
                if rate_td:
                    defaults['rate'] = rate_td.text.strip()
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ åŸºé‡‘ {fund_code} ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ åŸºé‡‘ {fund_code} æ•°æ®è§£æå¤±è´¥: {e}")
    
    FUND_INFO_CACHE[fund_code] = defaults
    return defaults


def clean_and_prepare_df(df, fund_code):
    """æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ï¼Œè¿”å›æ¸…ç†åçš„DataFrameå’Œå…¶æœ‰æ•ˆèµ·æ­¢æ—¥æœŸã€‚"""
    df.columns = df.columns.str.lower()
    df = df.rename(columns={'ç´¯è®¡å‡€å€¼': 'cumulative_net_value', 'date': 'date'})
    df['cumulative_net_value'] = pd.to_numeric(df['cumulative_net_value'], errors='coerce')
    
    # æç«¯å¼‚å¸¸å€¼ä¿®æ­£ (é’ˆå¯¹å¯èƒ½çš„è¾“å…¥é”™è¯¯ï¼Œä¾‹å¦‚å•ä½å‡€å€¼é”™è¾“ä¸ºç´¯è®¡å‡€å€¼)
    mask_high_error = df['cumulative_net_value'] > 50 
    if mask_high_error.any():
        print(f"âš ï¸ åŸºé‡‘ {fund_code} å‘ç°å¹¶ä¿®æ­£äº† {mask_high_error.sum()} ä¸ªæç«¯å‡€å€¼å¼‚å¸¸ç‚¹ï¼ˆ>50ï¼‰ã€‚")
        df.loc[mask_high_error, 'cumulative_net_value'] = df.loc[mask_high_error, 'cumulative_net_value'] / 100 
    
    df = df.dropna(subset=['cumulative_net_value', 'date'])

    # é›¶æˆ–è´Ÿå‡€å€¼æ¸…ç†
    mask_zero_or_negative = df['cumulative_net_value'] <= 0
    if mask_zero_or_negative.any():
        print(f"ğŸ’£ åŸºé‡‘ {fund_code} å‘ç° {mask_zero_or_negative.sum()} ä¸ªé›¶æˆ–è´Ÿå‡€å€¼ï¼Œå·²ç§»é™¤ã€‚")
        df.loc[mask_zero_or_negative, 'cumulative_net_value'] = np.nan
        df = df.dropna(subset=['cumulative_net_value'])
    
    try:
        df.loc[:, 'date'] = pd.to_datetime(df['date'])
    except:
        df.loc[:, 'date'] = df['date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if pd.notna(x) else np.nan)
        df = df.dropna(subset=['date'])

    df = df.sort_values(by='date').reset_index(drop=True)
    
    if len(df) < 2:
        return None, None, None
        
    start_date = df['date'].iloc[0]
    end_date = df['date'].iloc[-1]
    
    return df, start_date, end_date


def calculate_metrics(df, fund_code, period_prefix=''):
    """è®¡ç®—åŸºé‡‘çš„å„ç§é£é™©æ”¶ç›ŠæŒ‡æ ‡ã€‚"""
    global EPSILON
    
    if df is None or len(df) < 2:
        # å¦‚æœæ•°æ®ç‚¹ä¸è¶³ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«NaNå€¼çš„å­—å…¸
        metrics = {
            'åŸºé‡‘ä»£ç ': fund_code,
            f'{period_prefix}èµ·å§‹æ—¥æœŸ': df['date'].iloc[0].strftime('%Y-%m-%d') if len(df) > 0 else 'N/A',
            f'{period_prefix}ç»“æŸæ—¥æœŸ': df['date'].iloc[-1].strftime('%Y-%m-%d') if len(df) > 0 else 'N/A',
            f'{period_prefix}å¹´åŒ–æ”¶ç›Šç‡': np.nan,
            f'{period_prefix}å¹´åŒ–æ ‡å‡†å·®': np.nan,
            f'{period_prefix}æœ€å¤§å›æ’¤(MDD)': np.nan,
            f'{period_prefix}å¤æ™®æ¯”ç‡': np.nan,
        }
        for name in ROLLING_PERIODS:
             metrics[f'{period_prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = np.nan
        return metrics
        
    cumulative_net_value = df['cumulative_net_value']
    
    # --- 1. å¹´åŒ–æ”¶ç›Šç‡ (åŸºäºäº¤æ˜“æ—¥) ---
    if cumulative_net_value.iloc[0] <= 0:
        annual_return = np.nan
    else:
        total_return = (cumulative_net_value.iloc[-1] / cumulative_net_value.iloc[0]) - 1
        num_trading_days = len(cumulative_net_value) - 1
        
        if num_trading_days > 0:
            # å‡ ä½•å¹³å‡å¹´åŒ–
            annual_return = (1 + total_return) ** (TRADING_DAYS_PER_YEAR / num_trading_days) - 1
        else:
            annual_return = np.nan

    # --- 2. å¹´åŒ–æ ‡å‡†å·®å’Œæ—¥æ”¶ç›Šç‡ ---
    returns = cumulative_net_value.pct_change().dropna()
    
    annual_volatility = returns.std() * np.sqrt(TRADING_DAYS_PER_YEAR)
    
    # --- 3. æœ€å¤§å›æ’¤ ---
    max_drawdown = (cumulative_net_value / cumulative_net_value.expanding().max() - 1).min()

    # --- 4. å¤æ™®æ¯”ç‡ ---
    if annual_volatility > EPSILON: # é¿å…é™¤ä»¥é›¶
        sharpe_ratio = (annual_return - RISK_FREE_RATE) / annual_volatility
    else:
        sharpe_ratio = np.nan
        
    # --- 5. æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡ ---
    rolling_metrics = {}
    
    for name, period_days in ROLLING_PERIODS.items():
        if len(cumulative_net_value) < period_days:
            rolling_metrics[f'{period_prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = np.nan
            continue

        rolling_non_ann_returns = cumulative_net_value.pct_change(periods=period_days).dropna()
        compounding_factors = 1 + rolling_non_ann_returns
        compounding_factors = np.maximum(compounding_factors, EPSILON) # é¿å… log(<=0)

        log_returns = np.log(compounding_factors)
        mean_log_return = log_returns.mean()
        R_geo = np.exp(mean_log_return) - 1
        
        annualized_R_geo = (1 + R_geo) ** (TRADING_DAYS_PER_YEAR / period_days) - 1
        
        rolling_metrics[f'{period_prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({name})'] = annualized_R_geo

    metrics = {
        'åŸºé‡‘ä»£ç ': fund_code,
        f'{period_prefix}èµ·å§‹æ—¥æœŸ': df['date'].iloc[0].strftime('%Y-%m-%d'),
        f'{period_prefix}ç»“æŸæ—¥æœŸ': df['date'].iloc[-1].strftime('%Y-%m-%d'),
        f'{period_prefix}å¹´åŒ–æ”¶ç›Šç‡': annual_return,
        f'{period_prefix}å¹´åŒ–æ ‡å‡†å·®': annual_volatility,
        f'{period_prefix}æœ€å¤§å›æ’¤(MDD)': max_drawdown,
        f'{period_prefix}å¤æ™®æ¯”ç‡': sharpe_ratio,
        **rolling_metrics
    }
    
    return metrics


def main():
    if not os.path.isdir(FUND_DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®ç›®å½• '{FUND_DATA_DIR}'ã€‚è¯·åˆ›å»ºæ­¤ç›®å½•å¹¶å°†CSVæ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
        return

    csv_files = [f for f in os.listdir(FUND_DATA_DIR) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ é”™è¯¯ï¼š'{FUND_DATA_DIR}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ã€‚")
        return

    fund_codes = [f.split('.')[0] for f in csv_files]
    all_funds_data = {} # å­˜å‚¨æ¸…æ´—åçš„DF
    valid_start_dates = []
    valid_end_dates = []
    
    # é˜¶æ®µ 1: æ¸…æ´—æ•°æ®ï¼Œè®°å½•æœ‰æ•ˆèµ·æ­¢æœŸ
    print(f"--- é˜¶æ®µ 1/3: æ¸…æ´—æ•°æ®å¹¶ç¡®å®šå…±åŒåˆ†ææœŸ ---")
    
    for fund_code in fund_codes:
        file_path = os.path.join(FUND_DATA_DIR, f'{fund_code}.csv')
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç è¯»å–
            try:
                df_raw = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                df_raw = pd.read_csv(file_path, encoding='gbk')
            except pd.errors.ParserError:
                 df_raw = pd.read_csv(file_path, encoding='utf-8', sep='\t')
            
            df_clean, start_date, end_date = clean_and_prepare_df(df_raw.copy(), fund_code)
            
            if df_clean is not None:
                all_funds_data[fund_code] = df_clean
                valid_start_dates.append(start_date)
                valid_end_dates.append(end_date)
            else:
                print(f"âš ï¸ åŸºé‡‘ {fund_code} æ•°æ®ä¸è¶³æˆ–æ— æ•ˆï¼Œå·²è·³è¿‡ã€‚")
        
        except Exception as e:
            print(f"âŒ åŸºé‡‘ {fund_code} æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    if not all_funds_data:
        print("æ‰€æœ‰åŸºé‡‘æ•°æ®å¤„ç†å‡å¤±è´¥ã€‚")
        return

    # ç¡®å®šå…±åŒåˆ†ææœŸ
    latest_start = max(valid_start_dates) if valid_start_dates else None
    earliest_end = min(valid_end_dates) if valid_end_dates else None

    # æ£€æŸ¥å…±åŒæœŸæ˜¯å¦æœ‰æ•ˆï¼ˆè‡³å°‘éœ€è¦ 2 ä¸ªæ•°æ®ç‚¹ï¼‰
    min_days_required = 2 
    
    if latest_start is None or earliest_end is None or (earliest_end - latest_start).days < min_days_required:
        print("\nâŒ è­¦å‘Šï¼šæ— æ³•ç¡®å®šæœ‰æ•ˆçš„å…±åŒåˆ†ææœŸã€‚å°†è½¬è€Œè®¡ç®—æ‰€æœ‰åŸºé‡‘çš„å…¨å†å²æŒ‡æ ‡ã€‚")
        
        # å¦‚æœå…±åŒæœŸæ— æ•ˆï¼Œè®¡ç®—å…¨å†å²æŒ‡æ ‡
        common_metrics_list = []
        for fund_code, df in all_funds_data.items():
            metrics = calculate_metrics(df, fund_code, period_prefix='å…¨å†å²')
            if metrics:
                common_metrics_list.append(metrics)
        
        common_period_info = "æ‰€æœ‰åŸºé‡‘çš„å…¨å†å²æŒ‡æ ‡ (å…±åŒåˆ†ææœŸæ— æ•ˆ)"

    else:
        # å…±åŒåˆ†ææœŸæœ‰æ•ˆï¼Œæ‰§è¡Œç¬¬äºŒé˜¶æ®µè®¡ç®—
        common_period_info = f"æ‰€æœ‰åŸºé‡‘å…±åŒåˆ†ææœŸï¼š{latest_start.strftime('%Y-%m-%d')} åˆ° {earliest_end.strftime('%Y-%m-%d')}"
        print(f"\nâœ… å…±åŒåˆ†ææœŸç¡®å®šä¸ºï¼š{common_period_info}")
        
        # é˜¶æ®µ 2: è¿‡æ»¤æ•°æ®å¹¶è®¡ç®—å…±åŒåˆ†ææœŸæŒ‡æ ‡
        print("\n--- é˜¶æ®µ 2/3: è®¡ç®—å…±åŒåˆ†ææœŸå†…çš„æŒ‡æ ‡ ---")
        common_metrics_list = []
        
        for fund_code, df in all_funds_data.items():
            # è¿‡æ»¤æ•°æ®åˆ°å…±åŒåˆ†ææœŸå†…
            df_common = df[(df['date'] >= latest_start) & (df['date'] <= earliest_end)].copy()
            
            # æ£€æŸ¥å…±åŒæœŸå†…æ•°æ®ç‚¹æ˜¯å¦è¶³å¤Ÿ
            if len(df_common) < min_days_required:
                 print(f"âš ï¸ åŸºé‡‘ {fund_code} åœ¨å…±åŒæœŸå†…æ•°æ®ç‚¹ ({len(df_common)}ä¸ª) ä¸è¶³ï¼Œè·³è¿‡å…±åŒæœŸè®¡ç®—ã€‚")
                 metrics = {'åŸºé‡‘ä»£ç ': fund_code}
                 for col in ['èµ·å§‹æ—¥æœŸ', 'ç»“æŸæ—¥æœŸ', 'å¹´åŒ–æ”¶ç›Šç‡', 'å¹´åŒ–æ ‡å‡†å·®', 'æœ€å¤§å›æ’¤(MDD)', 'å¤æ™®æ¯”ç‡'] + [f'å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡({p})' for p in ROLLING_PERIODS]:
                    metrics[f'å…±åŒæœŸ{col}'] = np.nan
                 common_metrics_list.append(metrics)
                 continue

            metrics = calculate_metrics(df_common, fund_code, period_prefix='å…±åŒæœŸ')
            if metrics:
                common_metrics_list.append(metrics)
        
    final_metrics_df = pd.DataFrame(common_metrics_list)
    
    # é˜¶æ®µ 3: è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ (å¤šçº¿ç¨‹)
    fund_codes_to_fetch = final_metrics_df['åŸºé‡‘ä»£ç '].tolist()
    print(f"\n--- é˜¶æ®µ 3/3: å¤šçº¿ç¨‹è·å– {len(fund_codes_to_fetch)} æ”¯åŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯ ---")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_code = {executor.submit(fetch_fund_info, code): code for code in fund_codes_to_fetch}
        _ = [future.result() for future in concurrent.futures.as_completed(future_to_code)]

    # é˜¶æ®µ 4: æ•´åˆå’Œè¾“å‡º
    print("\n--- é˜¶æ®µ 4/4: æ•´åˆæ•°æ®å¹¶è¾“å‡ºç»“æœ ---")

    # ä»ç¼“å­˜ä¸­è·å–åŸºé‡‘ä¿¡æ¯å¹¶æ•´åˆ
    info_list = [FUND_INFO_CACHE[code] for code in final_metrics_df['åŸºé‡‘ä»£ç ']]
    info_df = pd.DataFrame(info_list).rename(columns={
        'name': 'åŸºé‡‘ç®€ç§°', 'size': 'èµ„äº§è§„æ¨¡', 'type': 'åŸºé‡‘ç±»å‹', 
        'daily_growth': 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'net_value': 'æœ€æ–°å‡€å€¼', 'rate': 'ç®¡ç†è´¹ç‡'
    })
    
    # é‡ç½®ç´¢å¼•å¹¶æ‹¼æ¥
    info_df.index = final_metrics_df.index
    final_df = pd.concat([info_df, final_metrics_df], axis=1)
    
    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”å’Œæ•°å­—
    sharpe_col_candidates = [col for col in final_df.columns if 'å¤æ™®æ¯”ç‡' in col]
    sharpe_col = sharpe_col_candidates[0] if sharpe_col_candidates else None
    
    if sharpe_col:
        
        # åˆ›å»ºä¸€ä¸ªç”¨äºæ’åºçš„ä¸´æ—¶æ•°å­—åˆ—ï¼ŒåŸºäºå…±åŒæœŸ/å…¨å†å²çš„å¤æ™®æ¯”ç‡
        final_df[f'{sharpe_col}_Num'] = final_df[sharpe_col].replace({'N/A': np.nan}).astype(float)
        
        for col in final_df.columns:
            if ('æ”¶ç›Šç‡' in col or 'æ ‡å‡†å·®' in col or 'å›æ’¤' in col) and col != sharpe_col:
                def format_pct(x):
                    if pd.isna(x) or np.isinf(x):
                        return 'N/A'
                    return f'{x * 100:.2f}%'
                final_df[col] = final_df[col].apply(format_pct)
            elif sharpe_col in col:
                def format_sharpe(x):
                    if pd.isna(x) or np.isinf(x):
                        return 'N/A'
                    return f'{x:.3f}'
                final_df[col] = final_df[col].apply(format_sharpe)
            
        # æ’åºï¼ˆæŒ‰å…±åŒæœŸ/å…¨å†å²å¤æ™®æ¯”ç‡é™åºï¼‰
        final_df = final_df.sort_values(by=f'{sharpe_col}_Num', ascending=False).drop(columns=[f'{sharpe_col}_Num']).reset_index(drop=True)
    
    # åˆ›å»ºå…±åŒåˆ†ææœŸä¿¡æ¯è¡Œ
    period_info_row = pd.Series(
        {'åŸºé‡‘ç®€ç§°': common_period_info, 'åŸºé‡‘ä»£ç ': 'åˆ†ææœŸä¿¡æ¯'},
        index=final_df.columns
    ).to_frame().T
    
    final_output = pd.concat([period_info_row, final_df], ignore_index=True)
    
    # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡® (å°† 'åŸºé‡‘ä»£ç ' å’Œ 'åŸºé‡‘ç®€ç§°' æå‰)
    prefix = 'å…±åŒæœŸ' if 'å…±åŒæœŸå¹´åŒ–æ”¶ç›Šç‡' in final_output.columns else 'å…¨å†å²'
    target_columns = [
        'åŸºé‡‘ä»£ç ', 'åŸºé‡‘ç®€ç§°', 'èµ„äº§è§„æ¨¡', 'åŸºé‡‘ç±»å‹', 'æœ€æ–°æ—¥æ¶¨è·Œå¹…', 'æœ€æ–°å‡€å€¼', 
        'ç®¡ç†è´¹ç‡', f'{prefix}èµ·å§‹æ—¥æœŸ', f'{prefix}ç»“æŸæ—¥æœŸ', 
        f'{prefix}å¹´åŒ–æ”¶ç›Šç‡', f'{prefix}å¹´åŒ–æ ‡å‡†å·®', f'{prefix}æœ€å¤§å›æ’¤(MDD)', 
        f'{prefix}å¤æ™®æ¯”ç‡', f'{prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1æœˆ)', 
        f'{prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1å­£åº¦)', f'{prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(åŠå¹´)', 
        f'{prefix}å¹³å‡æ»šåŠ¨å¹´åŒ–æ”¶ç›Šç‡(1å¹´)'
    ]
    
    # é‡æ–°æ’åˆ—åˆ—ï¼Œå¦‚æœæŸä¸ªåˆ—ç¼ºå¤±åˆ™å¿½ç•¥
    final_output = final_output[[col for col in target_columns if col in final_output.columns]]

    # ä½¿ç”¨ utf_8_sig ç¼–ç ä»¥ç¡®ä¿ Excel ä¸­æ–‡ä¸ä¹±ç 
    final_output.to_csv(OUTPUT_FILE, index=False, encoding='utf_8_sig')
    print(f"\nâœ… æˆåŠŸï¼šåˆ†æç»“æœå·²ä¿å­˜è‡³ {os.path.abspath(OUTPUT_FILE)}")
    
if __name__ == '__main__':
    main()
