# fund_data_collector_final.py (æœ€ç»ˆä¿®æ­£ç‰ˆ - å®Œæ•´å¹¶è¡Œ)

import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os
import time
import json
import random # å¼•å…¥ random ç”¨äºç”Ÿæˆéšæœºå»¶è¿Ÿ
from multiprocessing.dummy import Pool as ThreadPool # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œç½‘ç»œI/Oå¯†é›†å‹ä»»åŠ¡

# --- é…ç½® ---
FUND_CODES_FILE = "Cç±».txt"
# ç›´æ¥è¯·æ±‚æ•°æ®æ¥å£ï¼Œè¯¥æ¥å£è¿”å›åŒ…å«æ‰€æœ‰æŒä»“è¡¨æ ¼çš„HTMLç‰‡æ®µ
BASE_DATA_URL = "http://fundf10.eastmoney.com/FundArchivesDatas.aspx?type=ccmx&code={fund_code}&qdii=&sdate=&edate=&rt={timestamp}"
# è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°ã€‚å»ºè®®ä¿æŒåœ¨ 10 å·¦å³
MAX_WORKERS = 10 

# --- å·¥å…·å‡½æ•° ---

def get_output_dir():
    """è¿”å›å½“å‰çš„å¹´/æœˆç›®å½• (ä¸Šæµ·æ—¶åŒº)"""
    # ç¡®ä¿ä½¿ç”¨æ—¶åŒºæ„ŸçŸ¥æ—¶é—´
    cst_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8)))
    return os.path.join(cst_time.strftime("%Y"), cst_time.strftime("%m"))

def fetch_holding_data(fund_code):
    """æŠ“å–æŒ‡å®šåŸºé‡‘ä»£ç çš„æŒä»“è¡¨æ ¼ HTML å†…å®¹ï¼Œè¿”å›HTMLç‰‡æ®µæˆ– Noneã€‚"""
    timestamp = time.time() * 1000
    url = BASE_DATA_URL.format(fund_code=fund_code, timestamp=timestamp)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': f'http://fundf10.eastmoney.com/ccmx_{fund_code}.html'
    }
    
    try:
        # è®¾ç½®è¶…æ—¶
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'

        if response.status_code == 200:
            text = response.text.strip()
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä»¥é¢„æœŸæ ¼å¼å¼€å¤´
            if text.startswith('var apidata='):
                # æå– JSON å­—ç¬¦ä¸²éƒ¨åˆ†
                json_str = text.split('=', 1)[1].rstrip(';')
                
                try:
                    data = json.loads(json_str)
                    return data.get('content')
                except json.JSONDecodeError:
                    # å¢å¼ºçš„é”™è¯¯å¤„ç†
                    print(f"[{fund_code}] é”™è¯¯: æ— æ³•è§£æè¿”å›çš„ JSON å†…å®¹ã€‚åŸå§‹å†…å®¹å¯èƒ½è¢«æœåŠ¡å™¨æˆªæ–­æˆ–æ ¼å¼é”™è¯¯ã€‚")
                    return None
            else:
                # æœåŠ¡å™¨è¿”å›äº† 200ï¼Œä½†å†…å®¹ä¸æ˜¯æ•°æ®æ¥å£æ ¼å¼ï¼Œå¯èƒ½æ˜¯åçˆ¬æœºåˆ¶è§¦å‘
                print(f"[{fund_code}] é”™è¯¯: æ•°æ®æ¥å£è¿”å›æ ¼å¼ä¸æ­£ç¡® (æœªä»¥ 'var apidata=' å¼€å¤´)ã€‚å¯èƒ½è¢«æœåŠ¡å™¨é™åˆ¶ã€‚")
                print(f"[{fund_code}] åŸå§‹å†…å®¹å‰200å­—ç¬¦: {text[:200]}")
                return None
        else:
            print(f"[{fund_code}] æŠ“å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"[{fund_code}] æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None

def parse_and_save_data(fund_code, html_content):
    """è§£æ HTML å†…å®¹ï¼Œæå–æŒä»“è¡¨æ ¼æ•°æ®ï¼Œå¹¶ä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚"""
    if not html_content:
        return

    soup = BeautifulSoup(html_content, 'html.parser')
    tables = soup.find_all('table', class_='w780')
    
    if not tables:
        print(f"[{fund_code}] æœªæ‰¾åˆ°æŒä»“è¡¨æ ¼ã€‚")
        return

    # åªå¤„ç†ç¬¬ä¸€ä¸ªè¡¨æ ¼ï¼ˆæœ€æ–°å­£åº¦æ•°æ®ï¼‰
    table = tables[0]
    
    try:
        # 1. æå–æ ‡é¢˜
        title_tag = table.find_previous_sibling('h4')
        title = f"{fund_code}_è‚¡ç¥¨æŠ•èµ„æ˜ç»†"
        if title_tag:
            raw_title = title_tag.text.strip().replace('\r\n', ' ').replace('\n', ' ').replace('\xa0', ' ')
            
            # æå–åŸºé‡‘åå’Œå­£åº¦ä¿¡æ¯
            parts = raw_title.split(' ')
            # åŒ¹é… "åŸºé‡‘å 2025å¹´3å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†"
            if len(parts) >= 3 and ('å­£åº¦' in parts[-2] or 'å¹´åº¦' in parts[-2]):
                 title = f"{parts[0]}_{parts[-3]}{parts[-2]}{parts[-1]}"
            else:
                 title = raw_title.replace(' ', '_')

        
        # 2. æå–è¡Œæ•°æ®
        data_rows = []
        for row in table.find_all('tr')[1:]: # è·³è¿‡è¡¨å¤´è¡Œ
            cols = [col.text.strip().replace('\xa0', '').replace('\n', '').replace(' ', '') for col in row.find_all(['td'])]
            
            # æ ‡å‡†åŒ–æ•°æ®åˆ—
            if len(cols) == 10: # å« æœ€æ–°ä»·/æ¶¨è·Œå¹…
                # åºå·, ä»£ç , åç§°, æœ€æ–°ä»·, æ¶¨è·Œå¹…, å å‡€å€¼æ¯”ä¾‹, æŒè‚¡æ•°, æŒä»“å¸‚å€¼
                data_rows.append([cols[0], cols[1], cols[2], cols[3], cols[4], cols[6], cols[7], cols[8], cols[9]])
            elif len(cols) == 8: # ä¸å« æœ€æ–°ä»·/æ¶¨è·Œå¹…
                # å¡«å…… 'æœ€æ–°ä»·' å’Œ 'æ¶¨è·Œå¹…' ä¸ºç©ºå­—ç¬¦ä¸²
                data_rows.append([cols[0], cols[1], cols[2], '', '', cols[4], cols[5], cols[6], cols[7]])
            else:
                continue

        # è°ƒæ•´è¡¨å¤´ä»¥åŒ¹é…æˆ‘ä»¬æœ€ç»ˆä¿ç•™çš„åˆ—
        final_headers = ['åºå·', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'å å‡€å€¼æ¯”ä¾‹', 'æŒè‚¡æ•°ï¼ˆä¸‡è‚¡ï¼‰', 'æŒä»“å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰']
        
        df = pd.DataFrame(data_rows, columns=final_headers)
        
        # ç”Ÿæˆæ—¶é—´æˆ³å’Œæ–‡ä»¶å
        cst_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8)))
        timestamp = cst_time.strftime("%Y%m%d%H%M%S")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_dir = get_output_dir()
        os.makedirs(output_dir, exist_ok=True)
        
        filename = f"{title}_{timestamp}.csv"
        filepath = os.path.join(output_dir, filename)
        
        df.to_csv(filepath, index=False, encoding='utf_8_sig')
        print(f"[{fund_code}] æˆåŠŸä¿å­˜æ•°æ®åˆ°: {filepath}")
        
    except Exception as e:
        print(f"[{fund_code}] è§£ææˆ–ä¿å­˜è¡¨æ ¼æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# å¹¶è¡Œå¤„ç†å‡½æ•°
def process_fund(fund_code):
    """å•ä¸ªåŸºé‡‘ä»£ç çš„å®Œæ•´å¤„ç†æµç¨‹ã€‚"""
    
    # ğŸ’¥ CRITICAL FIX: åœ¨æ¯ä¸ªçº¿ç¨‹å¼€å§‹å‰æ·»åŠ ä¸€ä¸ªçŸ­æš‚çš„éšæœºå»¶è¿Ÿ
    # è¿™æå¤§åœ°å‡å°‘äº†è¢«é€Ÿç‡é™åˆ¶çš„å¯èƒ½æ€§ã€‚
    time.sleep(random.uniform(0.1, 1.0)) 
    
    print(f"[{fund_code}] å¼€å§‹æŠ“å–...")
    html_content = fetch_holding_data(fund_code)
    
    if html_content:
        parse_and_save_data(fund_code, html_content)
    else:
        print(f"[{fund_code}] æŠ“å–å¤±è´¥ï¼Œè·³è¿‡è§£æã€‚")
    # çº¿ç¨‹å¹¶è¡Œï¼Œä¸éœ€è¦åœ¨æœ€åæ·»åŠ  time.sleep

# ä¸»è¿è¡Œé€»è¾‘
def main():
    print("--- å¼€å§‹è¿è¡ŒåŸºé‡‘æ•°æ®æ”¶é›†è„šæœ¬ (å¹¶è¡Œæ¨¡å¼) ---")
    
    # 1. è¯»å–åŸºé‡‘ä»£ç 
    fund_codes = []
    try:
        with open(FUND_CODES_FILE, 'r') as f:
            fund_codes = [line.strip() for line in f if line.strip() and line.strip() != 'code']
        
        if not fund_codes:
            print("é”™è¯¯: åŸºé‡‘ä»£ç æ–‡ä»¶ä¸ºç©ºæˆ–åªåŒ…å«æ ‡é¢˜è¡Œã€‚")
            return

        print(f"è¯»å–åˆ° {len(fund_codes)} ä¸ªåŸºé‡‘ä»£ç ï¼Œå°†ä½¿ç”¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†ã€‚")
    except FileNotFoundError:
        print(f"é”™è¯¯: åŸºé‡‘ä»£ç æ–‡ä»¶ '{FUND_CODES_FILE}' æœªæ‰¾åˆ°ã€‚")
        return

    # 2. å¹¶è¡Œå¤„ç†
    pool = ThreadPool(MAX_WORKERS)
    
    # ä½¿ç”¨ map å‡½æ•°å°† process_fund åº”ç”¨åˆ° fund_codes åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
    pool.map(process_fund, fund_codes)
    
    pool.close()
    pool.join()
    
    print("\n--- è„šæœ¬è¿è¡Œç»“æŸ ---")

if __name__ == "__main__":
    main()
