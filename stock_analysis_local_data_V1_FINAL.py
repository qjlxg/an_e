# stock_analysis_local_data_V1_FINAL.py - ä»æœ¬åœ°æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œåˆ†æ

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor
import time
import akshare as ak # ä»…ä¿ç•™å¯¼å…¥ï¼Œä½†ä¸å†ä½¿ç”¨å…¶æ•°æ®è·å–åŠŸèƒ½
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
import os # æ–°å¢å¯¼å…¥ os

warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)


# --- AkShare å…¨å±€é…ç½® (ä¿ç•™ä½†ä¸æ‰§è¡Œæ•°æ®è·å–) ---
# try:
#     ak.set_time_out(30) # ç§»é™¤ AkShare é…ç½®ï¼Œå› ä¸ºä¸å†ä¾èµ–ç½‘ç»œè·å–
# except Exception as e:
#     print(f"è­¦å‘Šï¼šè®¾ç½® AkShare å…¨å±€è¶…æ—¶å¤±è´¥ï¼š{e}")


# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
# ä¿®æ”¹ä¸ºæœ¬åœ°æ•°æ®æ–‡ä»¶ç›®å½•å’Œåˆ†æç»“æœè¾“å‡ºç›®å½•
INPUT_DIR = "stock_data" # å‡è®¾åŸå§‹æ•°æ®æ–‡ä»¶æ”¾åœ¨æ­¤ç›®å½•
OUTPUT_DIR = "analyzed_data" # åˆ†æç»“æœè¾“å‡ºåˆ°æ­¤ç›®å½•

DEFAULT_START_DATE = '1990-01-01' # æ‰©å¤§èµ·å§‹æ—¥æœŸä»¥ç¡®ä¿åˆ†æå†å²å®Œæ•´æ€§
INDICATOR_LOOKBACK_DAYS = 30
LOCK_FILE = "stock_analysis.lock"

MAX_WORKERS = 1
MAX_RETRIES = 0 # ä¸å†éœ€è¦é‡è¯•

# --- æŒ‡æ•°åˆ—è¡¨åŠä»£ç ç»“æ„ (ä¿æŒä¸å˜) ---
INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1},
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0},
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1},
    '000300': {'name': 'æ²ªæ·±300', 'market': 1},
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1},
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1},
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0},
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1},
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0},
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1},
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1},
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0},
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}
SW_INDUSTRY_DICT = {'801010':'å†œæ—ç‰§æ¸”','801020':'é‡‡æ˜','801030':'åŒ–å·¥','801040':'é’¢é“','801050':'æœ‰è‰²é‡‘å±','801080':'ç”µå­','801110':'å®¶ç”¨ç”µå™¨','801120':'é£Ÿå“é¥®æ–™','801130':'çººç»‡æœè£…','801140':'è½»å·¥åˆ¶é€ ','801150':'åŒ»è¯ç”Ÿç‰©','801160':'å…¬ç”¨äº‹ä¸š','801170':'äº¤é€šè¿è¾“','801180':'æˆ¿åœ°äº§','801200':'å•†ä¸šè´¸æ˜“','801210':'ä¼‘é—²æœåŠ¡','801230':'ç»¼åˆ','801710':'å»ºç­‘ææ–™','801720':'å»ºç­‘è£…é¥°','801730':'ç”µæ°”è®¾å¤‡','801740':'å›½é˜²å†›å·¥','801750':'è®¡ç®—æœº','801760':'ä¼ åª’','801770':'é€šä¿¡','801780':'é“¶è¡Œ','801790':'éé“¶é‡‘è','801880':'æ±½è½¦','801890':'æœºæ¢°è®¾å¤‡','801060':'å»ºç­‘å»ºæ','801070':'æœºæ¢°è®¾å¤‡','801090':'äº¤è¿è®¾å¤‡','801190':'é‡‘èæœåŠ¡','801100':'ä¿¡æ¯è®¾å¤‡','801220':'ä¿¡æ¯æœåŠ¡'}
CS_INDUSTRY_DICT = {}
WIND_INDUSTRY_DICT = {}

def get_pytdx_market(code):
    code = str(code)
    if code.startswith('00') or code.startswith('88') or code.startswith('801') or code.startswith('CI005'):
        return 1
    elif code.startswith('399'):
        return 0
    return 1

def merge_industry_indexes(index_list, industry_dict, prefix=""):
    for code, name in industry_dict.items():
        pytdx_code = code.split('.')[0]
        if pytdx_code not in index_list:
            index_list[pytdx_code] = {
                'name': f'{prefix}{name}',
                'market': get_pytdx_market(pytdx_code)
            }
    return index_list

INDEX_LIST = merge_industry_indexes(INDEX_LIST, SW_INDUSTRY_DICT, prefix="ç”³ä¸‡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, CS_INDUSTRY_DICT, prefix="ä¸­ä¿¡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, WIND_INDUSTRY_DICT, prefix="ä¸‡å¾—ä¸€çº§_")

# --- æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis_local.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---
def calculate_full_technical_indicators(df):
    if df.empty: return df
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    price_cols = ['open', 'close', 'high', 'low', 'volume']
    for col in price_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¡Œè¿›è¡ŒæŒ‡æ ‡è®¡ç®—
    if len(df) < max(20, 14, 9): # ç®€å•æ£€æŸ¥æ‰€éœ€çš„æœ€å°é•¿åº¦
        logger.warning(f"Â  Â  - æ•°æ®é‡ä¸è¶³ ({len(df)} è¡Œ)ï¼Œè·³è¿‡æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€‚")
        return df.reset_index()

    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True); df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True); df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True); df = df.rename(columns={'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper', 'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'})
    df.ta.atr(length=14, append=True); df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True); df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    return df.reset_index()

def aggregate_and_analyze(df_raw_slice, freq, prefix):
    if df_raw_slice.empty: return pd.DataFrame()
    # å‡è®¾æœ¬åœ°æ•°æ®ä¸­æ²¡æœ‰æ¢æ‰‹ç‡ï¼Œæˆ–è€…ä¸ç”¨äºå‘¨/æœˆçº¿åˆæˆï¼Œè¿™é‡Œä¿æŒåŸæ ·
    if 'turnover_rate' not in df_raw_slice.columns:
        df_raw_slice['turnover_rate'] = float('nan') 
        
    df_raw_slice.index = pd.to_datetime(df_raw_slice.index)
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    if not agg_df.empty:
        agg_df = agg_df.reset_index().rename(columns={'index': 'date'})
        agg_df['date'] = agg_df['date'].dt.date
        agg_df = calculate_full_technical_indicators(agg_df)
        cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
        agg_df.set_index('date', inplace=True)
    return agg_df

# --- æ–°å¢ï¼šè¯»å–æœ¬åœ°å†å²æ•°æ®æ–‡ä»¶ ---
def load_local_history_data(code):
    """ä»æœ¬åœ° CSV æ–‡ä»¶è¯»å–æŒ‡æ•°å†å²æ•°æ®"""
    file_path = Path(INPUT_DIR) / f"{code.replace('.', '_')}.csv"
    logger.info(f"Â  Â  - å°è¯•ä»æœ¬åœ°æ–‡ä»¶è¯»å– {code}ï¼š{file_path.name}")
    
    if not file_path.exists():
        logger.error(f"Â  Â  - é”™è¯¯ï¼šæœ¬åœ°æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚")
        return pd.DataFrame()
        
    try:
        # å°è¯•è¯»å–æœ¬åœ° CSV
        # å‡è®¾æœ¬åœ° CSV æ–‡ä»¶çš„åˆ—ååŒ…å«: æ—¥æœŸ, è‚¡ç¥¨ä»£ç , å¼€ç›˜, æ”¶ç›˜, æœ€é«˜, æœ€ä½, æˆäº¤é‡, æˆäº¤é¢, æŒ¯å¹…, æ¶¨è·Œå¹…, æ¶¨è·Œé¢, æ¢æ‰‹ç‡
        # éœ€è¦è¿›è¡Œåˆ—åå’Œæ—¥æœŸæ ¼å¼çš„æ¸…æ´—ï¼Œä½¿å…¶ä¸ AkShare æ¥å£çš„è¾“å‡ºå…¼å®¹
        df = pd.read_csv(
            file_path, 
            dtype={'è‚¡ç¥¨ä»£ç ': str},
            parse_dates=['æ—¥æœŸ']
        )
        
        # æ¸…æ´—åˆ—å
        df.rename(columns={
            'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high', 'æœ€ä½': 'low', 
            'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount', 
            'æ¢æ‰‹ç‡': 'turnover_rate' # å¢åŠ æ¢æ‰‹ç‡æ”¯æŒ
        }, inplace=True)
        
        # ä»…ä¿ç•™å¿…éœ€çš„åˆ—
        required_cols = ['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate']
        df = df[[c for c in required_cols if c in df.columns]].copy()
        
        # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
        df['date'] = pd.to_datetime(df['date']).dt.date # è½¬æ¢ä¸º date å¯¹è±¡
        df.set_index('date', inplace=True)
        for col in ['open', 'close', 'high', 'low', 'volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        df.dropna(subset=['close'], inplace=True)
        df.sort_index(inplace=True)
        
        logger.info(f"Â  Â  - âœ… {code} æœ¬åœ°æ–‡ä»¶è¯»å–æˆåŠŸã€‚æ€»è¡Œæ•°: {len(df)}")
        return df
        
    except Exception as e:
        logger.error(f"Â  Â  - é”™è¯¯ï¼šè¯»å–æœ¬åœ°æ–‡ä»¶ {file_path.name} å¤±è´¥ã€‚é”™è¯¯: {e}")
        return pd.DataFrame()


# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä¿®æ”¹ä¸ºä»æœ¬åœ°æ–‡ä»¶è·å–) ---
def get_and_analyze_data_slice(code, start_date_to_process):
    try:
        # 1. ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ‰€æœ‰å†å²æ•°æ®
        df_full = load_local_history_data(code)

        if df_full.empty:
            logger.warning(f"Â  Â  - {code} æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
            return None

        # 2. ä»æŒ‡å®šæ—¥æœŸï¼ˆåŒ…å«å›æº¯æœŸï¼‰å¼€å§‹è¿›è¡Œåˆ†æ
        start_date = pd.to_datetime(start_date_to_process).date()
        df_raw = df_full[df_full.index >= start_date].copy()

        if df_raw.empty:
             logger.warning(f"Â  Â  - {code} åœ¨æŒ‡å®šèµ·å§‹æ—¥æœŸ {start_date} ä¹‹åæ— æ–°æ•°æ®ï¼Œæ— éœ€é‡æ–°åˆ†æã€‚")
             return None
             
        # 3. æ—¥çº¿æŒ‡æ ‡è®¡ç®—
        # å‡†å¤‡æ—¥çº¿è®¡ç®—æ‰€éœ€æ ¼å¼
        df_raw_processed = df_raw.reset_index().rename(columns={'index': 'date'})
        df_raw_processed['date'] = pd.to_datetime(df_raw_processed['date'])
        
        # è®¡ç®—æ—¥çº¿æŒ‡æ ‡
        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        
        # å‡†å¤‡å‘¨/æœˆ/å¹´çº¿è®¡ç®—æ‰€éœ€æ ¼å¼
        df_raw.index = pd.to_datetime(df_raw.index)
        
        # é‡å‘½åæ—¥çº¿åˆ—
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        
        # 4. å‘¨/æœˆ/å¹´çº¿èšåˆå’ŒæŒ‡æ ‡è®¡ç®—
        df_weekly = aggregate_and_analyze(df_raw.copy(), 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw.copy(), 'M', 'M')
        # df_yearly = aggregate_and_analyze(df_raw.copy(), 'Y', 'Y') # å¹´çº¿æ•°æ®é‡å¤ªå°‘ï¼Œæš‚æ—¶è·³è¿‡æˆ–ä¿ç•™
        df_yearly = aggregate_and_analyze(df_raw.copy(), 'Y', 'Y') 

        # 5. åˆå¹¶ç»“æœ
        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        
        logger.info(f"Â  Â  - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ (ä» {start_date} å¼€å§‹)ã€‚")
        return results.sort_index()
        
    except Exception as e:
        logger.error(f"Â  Â  - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

# --- ä¸»å¤„ç†å‡½æ•° (ä¿®æ”¹äº†ç›®å½•å) ---
def process_single_index(code_map):
    code = code_map['code']
    name = code_map['name']
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    # ç¡®å®šè¦ä»å“ªä¸ªæ—¥æœŸå¼€å§‹é‡æ–°è®¡ç®—æŒ‡æ ‡ (åŒ…å«å›æº¯æœŸ)
    start_date_to_process = DEFAULT_START_DATE
    df_old = pd.DataFrame()

    if output_path.exists():
        try:
            # è¯»å–æ—§çš„åˆ†æç»“æœ
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                # ç¡®å®šæœ¬æ¬¡éœ€è¦é‡æ–°è®¡ç®—çš„å¼€å§‹æ—¥æœŸ (åŒ…å«æŒ‡æ ‡å›æº¯æœŸ)
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_process = start_date_for_calc.strftime('%Y-%m-%d')
                
                # ç¡®ä¿ä¸ä¼šæ¯”é»˜è®¤èµ·å§‹æ—¥æœŸæ›´æ—©
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                    start_date_to_process = DEFAULT_START_DATE
                
                logger.info(f"Â  Â  - æ£€æµ‹åˆ°æ—§åˆ†æç»“æœï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚æœ¬æ¬¡åˆ†æä» {start_date_to_process} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"Â  Â  - æ—§åˆ†ææ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†é‡æ–°å…¨é‡åˆ†æã€‚")
        except Exception as e:
            logger.error(f"Â  Â  - è­¦å‘Šï¼šè¯»å–æ—§åˆ†ææ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†é‡æ–°å…¨é‡åˆ†æã€‚")
    else:
        logger.info(f"Â  Â  - åˆ†ææ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡åˆ†ææœ¬åœ°æ•°æ®ã€‚")
        
    # è·å–æ•°æ®å¹¶åˆ†æ (ä»æœ¬åœ°æ–‡ä»¶è·å–)
    df_new_analyzed = get_and_analyze_data_slice(code, start_date_to_process)
    
    # å¼‚å¸¸æˆ–æ— æ–°æ•°æ®å¤„ç†é€»è¾‘
    if df_new_analyzed is None:
        is_today_updated = False
        if not df_old.empty and pd.api.types.is_datetime64_any_dtype(df_old.index):
             today = datetime.now(shanghai_tz).date()
             is_today_updated = df_old.index.max().date() == today
        
        if is_today_updated:
            logger.info(f"Â  Â  - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        elif not df_old.empty:
             logger.warning(f"Â  Â  - {code} æœ¬åœ°æ–‡ä»¶æœªæ›´æ–°æˆ–åˆ†æå¤±è´¥ï¼Œä¿æŒåŸåˆ†ææ–‡ä»¶ã€‚")
        else:
             logger.error(f"Â  Â  - {code} æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨æˆ–åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ–‡ä»¶ã€‚")
        return False
        
    # åˆå¹¶æ–°æ—§æ•°æ®
    if not df_old.empty:
        # ç¡®ä¿ç´¢å¼•ä¸ºæ—¥æœŸå¯¹è±¡ï¼Œæ–¹ä¾¿æ¯”è¾ƒ
        df_old.index = pd.to_datetime(df_old.index)
        df_new_analyzed.index = pd.to_datetime(df_new_analyzed.index)

        # ä¿ç•™æ¯”æ–°åˆ†æç»“æœèµ·å§‹æ—¥æœŸæ›´æ—©çš„æ—§æ•°æ®
        old_data_to_keep = df_old[df_old.index.date < df_new_analyzed.index.min().date()]
    else:
        old_data_to_keep = pd.DataFrame()
        
    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    
    # å»é‡å¹¶æ’åº
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()
    
    logger.info(f"Â  Â  - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # ä¿å­˜ç»“æœ
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

def main():
    start_time = time.time()
    
    # ç¡®ä¿æœ¬åœ°æ•°æ®è¾“å…¥ç›®å½•å’Œåˆ†æç»“æœè¾“å‡ºç›®å½•å­˜åœ¨
    input_path = Path(INPUT_DIR)
    output_path = Path(OUTPUT_DIR)
    
    # åˆ›å»ºè¾“å…¥ç›®å½• (å¦‚æœä¸å­˜åœ¨ï¼Œæé†’ç”¨æˆ·å°†æ•°æ®æ”¾å…¥)
    if not input_path.exists():
         input_path.mkdir(exist_ok=True)
         logger.warning(f"æ³¨æ„ï¼šæœ¬åœ°æ•°æ®è¾“å…¥ç›®å½• {input_path.resolve()} ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºã€‚è¯·å°†åŸå§‹ CSV æ–‡ä»¶æ”¾å…¥æ­¤ç›®å½•ã€‚")
         # å¦‚æœè¾“å…¥ç›®å½•æ˜¯ç©ºçš„ï¼Œç›´æ¥é€€å‡º
         if not os.listdir(input_path):
             logger.error("æ•°æ®ç›®å½•ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚è¯·å°†åŸå§‹æ•°æ®æ–‡ä»¶æ”¾å…¥ `stock_data` ç›®å½•ã€‚")
             return
             
    output_path.mkdir(exist_ok=True)
    
    # é”æ–‡ä»¶é€»è¾‘ä¿æŒä¸å˜
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch()
    
    logger.info("â€”" * 50)
    logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (æœ¬åœ°æ•°æ®åˆ†ææ¨¡å¼)")
    logger.info(f"åˆ†æç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
    
    try:
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªæŒ‡æ•°...")
        successful = 0
        failed = 0
        
        # è¿‡æ»¤æ‰æœ¬åœ°ä¸å­˜åœ¨åŸå§‹æ–‡ä»¶çš„æŒ‡æ•°ï¼Œæé«˜æ•ˆç‡
        available_jobs = []
        for code, data in INDEX_LIST.items():
            file_name = f"{code.replace('.', '_')}.csv"
            file_path = input_path / file_name
            if file_path.exists():
                available_jobs.append({'code': code, **data})
            else:
                logger.warning(f"Â  Â  - è·³è¿‡æŒ‡æ•° {code} ({data['name']})ï¼šæœ¬åœ°æ•°æ®æ–‡ä»¶ {file_name} ä¸å­˜åœ¨äº {INPUT_DIR} ç›®å½•ã€‚")
                
        jobs = available_jobs

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(process_single_index, job): job for job in jobs}
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
                    
        end_time = time.time()
        elapsed_time = end_time - start_time
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed + (len(INDEX_LIST) - len(jobs))} ä¸ª (å…¶ä¸­è·³è¿‡å› ç¼ºå°‘æœ¬åœ°æ–‡ä»¶ï¼š{len(INDEX_LIST) - len(jobs)})ã€‚")
        
    finally:
        lock_file_path.unlink(missing_ok=True)
        logger.info("é”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
